# Story 5.6: Alert System for Critical Issues

**Epic**: Epic 5 - Observability & Production Readiness  
**Category**: Alerting & Notification  
**Priority**: MVP Critical (Partial)  
**Status**: Draft

## Acceptance Criteria

- [ ] Alert triggers: escalation after 3 retries, system error (uncaught exception), API rate limit hit, event store write failure
- [ ] Alert channels: CLI output (if running), webhook (POST to configured URL), email (if configured)
- [ ] Alert payload includes: severity (critical/warning/info), title, description, correlation ID, timestamp, suggested action
- [ ] Alert rate limiting: no more than 5 alerts per minute (prevent spam)
- [ ] Alert history stored in database for review
- [ ] Alert delivery tested with mock webhook endpoint
- [ ] Alert system supports configuration of custom alert rules
- [ ] Alert acknowledgment and resolution tracking
- [ ] Performance impact: <2ms overhead per alert evaluation
- [ ] Integration with metrics and logging systems

## Technical Context

### Alert System Architecture

Based on Epic 5.6 requirements and existing observability infrastructure:

```typescript
// Alert System Interface
interface AlertManager {
  // Alert evaluation
  evaluateAlert(metric: MetricData, rule: AlertRule): Promise<Alert | null>;
  evaluateAlerts(metrics: MetricData[]): Promise<Alert[]>;

  // Alert delivery
  sendAlert(alert: Alert): Promise<AlertDeliveryResult[]>;

  // Alert management
  acknowledgeAlert(alertId: string, userId: string): Promise<void>;
  resolveAlert(alertId: string, userId: string, resolution: string): Promise<void>;

  // Alert history
  getAlertHistory(filters?: AlertFilters): Promise<Alert[]>;
  getActiveAlerts(): Promise<Alert[]>;
}

interface Alert {
  id: string;
  ruleId: string;
  severity: 'critical' | 'warning' | 'info';
  title: string;
  description: string;
  correlationId?: string;
  timestamp: Date;
  metadata: Record<string, unknown>;
  status: 'active' | 'acknowledged' | 'resolved';
  acknowledgedBy?: string;
  acknowledgedAt?: Date;
  resolvedBy?: string;
  resolvedAt?: Date;
  resolution?: string;
  deliveryResults: AlertDeliveryResult[];
}

interface AlertRule {
  id: string;
  name: string;
  description: string;
  enabled: boolean;
  severity: 'critical' | 'warning' | 'info';
  condition: AlertCondition;
  cooldown: number; // seconds
  channels: AlertChannel[];
  metadata: Record<string, unknown>;
}

interface AlertCondition {
  metric: string;
  operator: 'gt' | 'lt' | 'eq' | 'ne' | 'gte' | 'lte';
  threshold: number;
  duration?: number; // seconds
  labels?: Record<string, string>;
}

interface AlertChannel {
  type: 'webhook' | 'email' | 'slack' | 'cli';
  config: Record<string, unknown>;
  enabled: boolean;
}

interface AlertDeliveryResult {
  channel: string;
  status: 'success' | 'failed' | 'pending';
  timestamp: Date;
  error?: string;
  retryCount: number;
}
```

### Alert Manager Implementation

```typescript
// packages/observability/src/alert-manager.ts
import { randomUUID } from 'crypto';
import { EventEmitter } from 'events';
import type {
  AlertManager,
  Alert,
  AlertRule,
  AlertCondition,
  AlertChannel,
  AlertDeliveryResult,
  AlertFilters,
} from './types';

export class TammaAlertManager extends EventEmitter implements AlertManager {
  private rules: Map<string, AlertRule> = new Map();
  private activeAlerts: Map<string, Alert> = new Map();
  private alertHistory: Alert[] = [];
  private rateLimiter: Map<string, number[]> = new Map();
  private channels: Map<string, AlertChannel> = new Map();

  constructor() {
    super();
    this.initializeDefaultRules();
    this.initializeDefaultChannels();
  }

  // Alert evaluation
  async evaluateAlert(metric: MetricData, rule: AlertRule): Promise<Alert | null> {
    if (!rule.enabled) return null;

    // Check if condition matches
    if (!this.matchesCondition(metric, rule.condition)) {
      return null;
    }

    // Check cooldown period
    if (this.isInCooldown(rule.id)) {
      return null;
    }

    // Check rate limiting
    if (this.isRateLimited(rule.id)) {
      return null;
    }

    // Create alert
    const alert: Alert = {
      id: randomUUID(),
      ruleId: rule.id,
      severity: rule.severity,
      title: this.generateAlertTitle(rule, metric),
      description: this.generateAlertDescription(rule, metric),
      correlationId: metric.labels?.correlationId,
      timestamp: new Date(),
      metadata: {
        metric: metric.name,
        value: metric.value,
        labels: metric.labels,
        rule: rule.name,
      },
      status: 'active',
      deliveryResults: [],
    };

    // Store alert
    this.activeAlerts.set(alert.id, alert);
    this.alertHistory.push(alert);

    // Update cooldown and rate limiting
    this.updateCooldown(rule.id);
    this.updateRateLimit(rule.id);

    // Emit alert event
    this.emit('alert', alert);

    return alert;
  }

  async evaluateAlerts(metrics: MetricData[]): Promise<Alert[]> {
    const alerts: Alert[] = [];

    for (const rule of this.rules.values()) {
      if (!rule.enabled) continue;

      for (const metric of metrics) {
        const alert = await this.evaluateAlert(metric, rule);
        if (alert) {
          alerts.push(alert);
        }
      }
    }

    return alerts;
  }

  // Alert delivery
  async sendAlert(alert: Alert): Promise<AlertDeliveryResult[]> {
    const rule = this.rules.get(alert.ruleId);
    if (!rule) {
      throw new Error(`Alert rule ${alert.ruleId} not found`);
    }

    const results: AlertDeliveryResult[] = [];

    for (const channel of rule.channels) {
      if (!channel.enabled) continue;

      const result = await this.deliverToChannel(alert, channel);
      results.push(result);
      alert.deliveryResults.push(result);
    }

    return results;
  }

  // Alert management
  async acknowledgeAlert(alertId: string, userId: string): Promise<void> {
    const alert = this.activeAlerts.get(alertId);
    if (!alert) {
      throw new Error(`Alert ${alertId} not found`);
    }

    alert.status = 'acknowledged';
    alert.acknowledgedBy = userId;
    alert.acknowledgedAt = new Date();

    this.emit('alertAcknowledged', alert);
  }

  async resolveAlert(alertId: string, userId: string, resolution: string): Promise<void> {
    const alert = this.activeAlerts.get(alertId);
    if (!alert) {
      throw new Error(`Alert ${alertId} not found`);
    }

    alert.status = 'resolved';
    alert.resolvedBy = userId;
    alert.resolvedAt = new Date();
    alert.resolution = resolution;

    this.activeAlerts.delete(alertId);
    this.emit('alertResolved', alert);
  }

  // Alert history
  async getAlertHistory(filters?: AlertFilters): Promise<Alert[]> {
    let filtered = [...this.alertHistory];

    if (filters) {
      if (filters.severity) {
        filtered = filtered.filter((alert) => alert.severity === filters.severity);
      }
      if (filters.status) {
        filtered = filtered.filter((alert) => alert.status === filters.status);
      }
      if (filters.startDate) {
        filtered = filtered.filter((alert) => alert.timestamp >= filters.startDate!);
      }
      if (filters.endDate) {
        filtered = filtered.filter((alert) => alert.timestamp <= filters.endDate!);
      }
      if (filters.ruleId) {
        filtered = filtered.filter((alert) => alert.ruleId === filters.ruleId);
      }
    }

    return filtered.sort((a, b) => b.timestamp.getTime() - a.timestamp.getTime());
  }

  async getActiveAlerts(): Promise<Alert[]> {
    return Array.from(this.activeAlerts.values()).sort(
      (a, b) => b.timestamp.getTime() - a.timestamp.getTime()
    );
  }

  // Rule management
  addRule(rule: AlertRule): void {
    this.rules.set(rule.id, rule);
  }

  updateRule(ruleId: string, updates: Partial<AlertRule>): void {
    const rule = this.rules.get(ruleId);
    if (!rule) {
      throw new Error(`Alert rule ${ruleId} not found`);
    }

    Object.assign(rule, updates);
  }

  removeRule(ruleId: string): void {
    this.rules.delete(ruleId);
  }

  // Private helper methods
  private matchesCondition(metric: MetricData, condition: AlertCondition): boolean {
    if (metric.name !== condition.metric) return false;
    if (condition.labels) {
      for (const [key, value] of Object.entries(condition.labels)) {
        if (metric.labels[key] !== value) return false;
      }
    }

    const value = metric.value;
    const threshold = condition.threshold;

    switch (condition.operator) {
      case 'gt':
        return value > threshold;
      case 'lt':
        return value < threshold;
      case 'eq':
        return value === threshold;
      case 'ne':
        return value !== threshold;
      case 'gte':
        return value >= threshold;
      case 'lte':
        return value <= threshold;
      default:
        return false;
    }
  }

  private isInCooldown(ruleId: string): boolean {
    // Implementation for cooldown checking
    return false; // Simplified
  }

  private isRateLimited(ruleId: string): boolean {
    const now = Date.now();
    const window = 60 * 1000; // 1 minute
    const maxAlerts = 5;

    const timestamps = this.rateLimiter.get(ruleId) || [];
    const recentTimestamps = timestamps.filter((ts) => now - ts < window);

    if (recentTimestamps.length >= maxAlerts) {
      return true;
    }

    return false;
  }

  private updateCooldown(ruleId: string): void {
    // Implementation for cooldown update
  }

  private updateRateLimit(ruleId: string): void {
    const now = Date.now();
    const timestamps = this.rateLimiter.get(ruleId) || [];
    timestamps.push(now);
    this.rateLimiter.set(ruleId, timestamps);
  }

  private generateAlertTitle(rule: AlertRule, metric: MetricData): string {
    return `${rule.name}: ${metric.name} is ${metric.value}`;
  }

  private generateAlertDescription(rule: AlertRule, metric: MetricData): string {
    return `Alert rule "${rule.name}" triggered by metric "${metric.name}" with value ${metric.value}. Threshold: ${rule.condition.threshold}.`;
  }

  private async deliverToChannel(
    alert: Alert,
    channel: AlertChannel
  ): Promise<AlertDeliveryResult> {
    const startTime = Date.now();

    try {
      switch (channel.type) {
        case 'webhook':
          await this.deliverWebhook(alert, channel.config);
          break;
        case 'email':
          await this.deliverEmail(alert, channel.config);
          break;
        case 'slack':
          await this.deliverSlack(alert, channel.config);
          break;
        case 'cli':
          this.deliverCLI(alert);
          break;
        default:
          throw new Error(`Unknown channel type: ${channel.type}`);
      }

      return {
        channel: channel.type,
        status: 'success',
        timestamp: new Date(),
        retryCount: 0,
      };
    } catch (error) {
      return {
        channel: channel.type,
        status: 'failed',
        timestamp: new Date(),
        error: error instanceof Error ? error.message : 'Unknown error',
        retryCount: 0,
      };
    }
  }

  private async deliverWebhook(alert: Alert, config: Record<string, unknown>): Promise<void> {
    const url = config.url as string;
    const secret = config.secret as string;

    const payload = {
      alert,
      timestamp: new Date().toISOString(),
    };

    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'X-Tamma-Signature': this.generateSignature(payload, secret),
      },
      body: JSON.stringify(payload),
    });

    if (!response.ok) {
      throw new Error(`Webhook delivery failed: ${response.status} ${response.statusText}`);
    }
  }

  private async deliverEmail(alert: Alert, config: Record<string, unknown>): Promise<void> {
    // Email delivery implementation
    // Would use nodemailer or similar service
  }

  private async deliverSlack(alert: Alert, config: Record<string, unknown>): Promise<void> {
    const webhookUrl = config.webhookUrl as string;

    const payload = {
      text: `ðŸš¨ ${alert.severity.toUpperCase()}: ${alert.title}`,
      attachments: [
        {
          color: this.getSeverityColor(alert.severity),
          fields: [
            { title: 'Description', value: alert.description, short: false },
            { title: 'Severity', value: alert.severity, short: true },
            { title: 'Correlation ID', value: alert.correlationId || 'N/A', short: true },
            { title: 'Timestamp', value: alert.timestamp.toISOString(), short: true },
          ],
        },
      ],
    };

    const response = await fetch(webhookUrl, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(payload),
    });

    if (!response.ok) {
      throw new Error(`Slack delivery failed: ${response.status} ${response.statusText}`);
    }
  }

  private deliverCLI(alert: Alert): void {
    const emoji = this.getSeverityEmoji(alert.severity);
    console.log(`${emoji} [${alert.severity.toUpperCase()}] ${alert.title}`);
    console.log(`  ${alert.description}`);
    if (alert.correlationId) {
      console.log(`  Correlation ID: ${alert.correlationId}`);
    }
    console.log(`  Timestamp: ${alert.timestamp.toISOString()}`);
    console.log();
  }

  private generateSignature(payload: any, secret: string): string {
    const crypto = require('crypto');
    const hmac = crypto.createHmac('sha256', secret);
    hmac.update(JSON.stringify(payload));
    return hmac.digest('hex');
  }

  private getSeverityColor(severity: string): string {
    switch (severity) {
      case 'critical':
        return 'danger';
      case 'warning':
        return 'warning';
      case 'info':
        return 'good';
      default:
        return 'good';
    }
  }

  private getSeverityEmoji(severity: string): string {
    switch (severity) {
      case 'critical':
        return 'ðŸ”´';
      case 'warning':
        return 'ðŸŸ¡';
      case 'info':
        return 'ðŸ”µ';
      default:
        return 'âšª';
    }
  }

  private initializeDefaultRules(): void {
    // Escalation after 3 retries
    this.addRule({
      id: 'escalation-retries',
      name: 'Escalation After 3 Retries',
      description: 'Alert when workflow escalates after 3 retry attempts',
      enabled: true,
      severity: 'critical',
      condition: {
        metric: 'escalations_total',
        operator: 'gte',
        threshold: 3,
        labels: { reason: 'retry_exhausted' },
      },
      cooldown: 300, // 5 minutes
      channels: [
        { type: 'cli', config: {}, enabled: true },
        { type: 'webhook', config: { url: process.env.ALERT_WEBHOOK_URL }, enabled: true },
      ],
      metadata: {},
    });

    // System error (uncaught exception)
    this.addRule({
      id: 'system-error',
      name: 'System Error',
      description: 'Alert on uncaught system exceptions',
      enabled: true,
      severity: 'critical',
      condition: {
        metric: 'errors_total',
        operator: 'gte',
        threshold: 1,
        labels: { severity: 'critical', component: 'system' },
      },
      cooldown: 60, // 1 minute
      channels: [
        { type: 'cli', config: {}, enabled: true },
        { type: 'webhook', config: { url: process.env.ALERT_WEBHOOK_URL }, enabled: true },
      ],
      metadata: {},
    });

    // API rate limit hit
    this.addRule({
      id: 'api-rate-limit',
      name: 'API Rate Limit Hit',
      description: 'Alert when API rate limits are exceeded',
      enabled: true,
      severity: 'warning',
      condition: {
        metric: 'api_rate_limits_total',
        operator: 'gte',
        threshold: 1,
      },
      cooldown: 300, // 5 minutes
      channels: [
        { type: 'cli', config: {}, enabled: true },
        { type: 'webhook', config: { url: process.env.ALERT_WEBHOOK_URL }, enabled: true },
      ],
      metadata: {},
    });

    // Event store write failure
    this.addRule({
      id: 'event-store-failure',
      name: 'Event Store Write Failure',
      description: 'Alert when event store writes fail',
      enabled: true,
      severity: 'critical',
      condition: {
        metric: 'event_store_write_errors_total',
        operator: 'gte',
        threshold: 1,
      },
      cooldown: 60, // 1 minute
      channels: [
        { type: 'cli', config: {}, enabled: true },
        { type: 'webhook', config: { url: process.env.ALERT_WEBHOOK_URL }, enabled: true },
      ],
      metadata: {},
    });
  }

  private initializeDefaultChannels(): void {
    // CLI channel
    this.channels.set('cli', {
      type: 'cli',
      config: {},
      enabled: true,
    });

    // Webhook channel
    if (process.env.ALERT_WEBHOOK_URL) {
      this.channels.set('webhook', {
        type: 'webhook',
        config: {
          url: process.env.ALERT_WEBHOOK_URL,
          secret: process.env.ALERT_WEBHOOK_SECRET,
        },
        enabled: true,
      });
    }
  }
}
```

### Alert Integration with Metrics

```typescript
// packages/observability/src/alert-integration.ts
import { TammaAlertManager } from './alert-manager';
import { PrometheusMetricsCollector } from './metrics';
import type { MetricData } from './types';

export class AlertIntegration {
  constructor(
    private alertManager: TammaAlertManager,
    private metrics: PrometheusMetricsCollector
  ) {
    this.setupMetricListeners();
  }

  private setupMetricListeners(): void {
    // Listen for metric updates and evaluate alerts
    this.metrics.on('metric', (metric: MetricData) => {
      this.evaluateAlertsForMetric(metric);
    });

    // Listen for alert events and update metrics
    this.alertManager.on('alert', (alert) => {
      this.recordAlertMetrics(alert);
    });

    this.alertManager.on('alertAcknowledged', (alert) => {
      this.recordAlertAcknowledgedMetrics(alert);
    });

    this.alertManager.on('alertResolved', (alert) => {
      this.recordAlertResolvedMetrics(alert);
    });
  }

  private async evaluateAlertsForMetric(metric: MetricData): Promise<void> {
    try {
      const alerts = await this.alertManager.evaluateAlerts([metric]);

      for (const alert of alerts) {
        await this.alertManager.sendAlert(alert);
      }
    } catch (error) {
      console.error('Failed to evaluate alerts for metric:', error);
    }
  }

  private recordAlertMetrics(alert: Alert): void {
    this.metrics.incrementCounter('alerts_total', {
      severity: alert.severity,
      rule: alert.ruleId,
    });

    this.metrics.incrementGauge('active_alerts', {
      severity: alert.severity,
    });
  }

  private recordAlertAcknowledgedMetrics(alert: Alert): void {
    this.metrics.incrementCounter('alerts_acknowledged_total', {
      severity: alert.severity,
      rule: alert.ruleId,
    });

    this.metrics.decrementGauge('active_alerts', {
      severity: alert.severity,
    });
  }

  private recordAlertResolvedMetrics(alert: Alert): void {
    this.metrics.incrementCounter('alerts_resolved_total', {
      severity: alert.severity,
      rule: alert.ruleId,
    });

    // Calculate resolution time
    if (alert.acknowledgedAt && alert.resolvedAt) {
      const resolutionTime = (alert.resolvedAt.getTime() - alert.acknowledgedAt.getTime()) / 1000;
      this.metrics.observeHistogram('alert_resolution_duration_seconds', resolutionTime, {
        severity: alert.severity,
        rule: alert.ruleId,
      });
    }
  }
}
```

### Alert API Endpoints

```typescript
// packages/api/src/alert-endpoints.ts
import { FastifyInstance, FastifyRequest, FastifyReply } from 'fastify';
import { TammaAlertManager } from '@tamma/observability';

export class AlertEndpoints {
  constructor(private alertManager: TammaAlertManager) {}

  registerRoutes(fastify: FastifyInstance): void {
    // Get active alerts
    fastify.get(
      '/alerts',
      {
        schema: {
          description: 'Get active alerts',
          tags: ['alerts'],
          querystring: {
            type: 'object',
            properties: {
              severity: { type: 'string', enum: ['critical', 'warning', 'info'] },
              limit: { type: 'number', minimum: 1, maximum: 100, default: 50 },
            },
          },
          response: {
            200: {
              type: 'array',
              items: {
                type: 'object',
                properties: {
                  id: { type: 'string' },
                  severity: { type: 'string' },
                  title: { type: 'string' },
                  description: { type: 'string' },
                  timestamp: { type: 'string' },
                  status: { type: 'string' },
                },
              },
            },
          },
        },
      },
      async (request: FastifyRequest, reply: FastifyReply) => {
        const { severity, limit } = request.query as any;

        let alerts = await this.alertManager.getActiveAlerts();

        if (severity) {
          alerts = alerts.filter((alert) => alert.severity === severity);
        }

        if (limit) {
          alerts = alerts.slice(0, limit);
        }

        return alerts;
      }
    );

    // Get alert history
    fastify.get(
      '/alerts/history',
      {
        schema: {
          description: 'Get alert history',
          tags: ['alerts'],
          querystring: {
            type: 'object',
            properties: {
              severity: { type: 'string', enum: ['critical', 'warning', 'info'] },
              status: { type: 'string', enum: ['active', 'acknowledged', 'resolved'] },
              startDate: { type: 'string', format: 'date-time' },
              endDate: { type: 'string', format: 'date-time' },
              limit: { type: 'number', minimum: 1, maximum: 1000, default: 100 },
            },
          },
        },
      },
      async (request: FastifyRequest, reply: FastifyReply) => {
        const filters = request.query as any;

        const alerts = await this.alertManager.getAlertHistory(filters);

        return alerts;
      }
    );

    // Acknowledge alert
    fastify.post(
      '/alerts/:alertId/acknowledge',
      {
        schema: {
          description: 'Acknowledge an alert',
          tags: ['alerts'],
          params: {
            type: 'object',
            properties: {
              alertId: { type: 'string' },
            },
          },
          body: {
            type: 'object',
            properties: {
              userId: { type: 'string' },
            },
          },
        },
      },
      async (request: FastifyRequest, reply: FastifyReply) => {
        const { alertId } = request.params as any;
        const { userId } = request.body as any;

        try {
          await this.alertManager.acknowledgeAlert(alertId, userId);
          return { success: true };
        } catch (error) {
          reply.code(404);
          return { error: 'Alert not found' };
        }
      }
    );

    // Resolve alert
    fastify.post(
      '/alerts/:alertId/resolve',
      {
        schema: {
          description: 'Resolve an alert',
          tags: ['alerts'],
          params: {
            type: 'object',
            properties: {
              alertId: { type: 'string' },
            },
          },
          body: {
            type: 'object',
            properties: {
              userId: { type: 'string' },
              resolution: { type: 'string' },
            },
          },
        },
      },
      async (request: FastifyRequest, reply: FastifyReply) => {
        const { alertId } = request.params as any;
        const { userId, resolution } = request.body as any;

        try {
          await this.alertManager.resolveAlert(alertId, userId, resolution);
          return { success: true };
        } catch (error) {
          reply.code(404);
          return { error: 'Alert not found' };
        }
      }
    );

    // Test alert delivery
    fastify.post(
      '/alerts/test',
      {
        schema: {
          description: 'Test alert delivery',
          tags: ['alerts'],
          body: {
            type: 'object',
            properties: {
              severity: { type: 'string', enum: ['critical', 'warning', 'info'] },
              title: { type: 'string' },
              description: { type: 'string' },
            },
          },
        },
      },
      async (request: FastifyRequest, reply: FastifyReply) => {
        const { severity, title, description } = request.body as any;

        const testAlert = {
          id: 'test-' + Date.now(),
          ruleId: 'test-rule',
          severity,
          title: title || 'Test Alert',
          description: description || 'This is a test alert',
          timestamp: new Date(),
          metadata: { test: true },
          status: 'active' as const,
          deliveryResults: [],
        };

        try {
          const results = await this.alertManager.sendAlert(testAlert);
          return { success: true, results };
        } catch (error) {
          reply.code(500);
          return { error: 'Failed to send test alert' };
        }
      }
    );
  }
}
```

## Implementation Tasks

### 1. Alert Configuration

```typescript
// packages/observability/src/alert-config.ts
export interface AlertConfig {
  enabled: boolean;
  rules: AlertRuleConfig[];
  channels: AlertChannelConfig[];
  rateLimit: {
    maxAlerts: number;
    windowSeconds: number;
  };
  retention: {
    historyDays: number;
    activeDays: number;
  };
}

export interface AlertRuleConfig {
  id: string;
  name: string;
  description: string;
  enabled: boolean;
  severity: 'critical' | 'warning' | 'info';
  condition: {
    metric: string;
    operator: string;
    threshold: number;
    duration?: number;
    labels?: Record<string, string>;
  };
  cooldown: number;
  channels: string[];
}

export const defaultAlertConfig: AlertConfig = {
  enabled: process.env.ALERTS_ENABLED !== 'false',
  rules: [
    {
      id: 'escalation-retries',
      name: 'Escalation After 3 Retries',
      description: 'Alert when workflow escalates after 3 retry attempts',
      enabled: true,
      severity: 'critical',
      condition: {
        metric: 'escalations_total',
        operator: 'gte',
        threshold: 3,
        labels: { reason: 'retry_exhausted' },
      },
      cooldown: 300,
      channels: ['cli', 'webhook'],
    },
  ],
  channels: [
    {
      type: 'cli',
      enabled: true,
      config: {},
    },
    {
      type: 'webhook',
      enabled: !!process.env.ALERT_WEBHOOK_URL,
      config: {
        url: process.env.ALERT_WEBHOOK_URL,
        secret: process.env.ALERT_WEBHOOK_SECRET,
      },
    },
  ],
  rateLimit: {
    maxAlerts: 5,
    windowSeconds: 60,
  },
  retention: {
    historyDays: 30,
    activeDays: 7,
  },
};
```

### 2. Alert Storage

```typescript
// packages/observability/src/alert-storage.ts
export class AlertStorage {
  constructor(private db: Database) {}

  async saveAlert(alert: Alert): Promise<void> {
    await this.db.query(
      `
      INSERT INTO alerts (
        id, rule_id, severity, title, description, correlation_id,
        timestamp, metadata, status, delivery_results
      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    `,
      [
        alert.id,
        alert.ruleId,
        alert.severity,
        alert.title,
        alert.description,
        alert.correlationId,
        alert.timestamp,
        JSON.stringify(alert.metadata),
        alert.status,
        JSON.stringify(alert.deliveryResults),
      ]
    );
  }

  async updateAlertStatus(
    alertId: string,
    status: string,
    userId?: string,
    resolution?: string
  ): Promise<void> {
    const updates = ['status = ?'];
    const values = [status];

    if (userId) {
      if (status === 'acknowledged') {
        updates.push('acknowledged_by = ?, acknowledged_at = ?');
        values.push(userId, new Date());
      } else if (status === 'resolved') {
        updates.push('resolved_by = ?, resolved_at = ?, resolution = ?');
        values.push(userId, new Date(), resolution);
      }
    }

    values.push(alertId);

    await this.db.query(
      `
      UPDATE alerts SET ${updates.join(', ')} WHERE id = ?
    `,
      values
    );
  }

  async getActiveAlerts(limit = 50): Promise<Alert[]> {
    const rows = await this.db.query(
      `
      SELECT * FROM alerts 
      WHERE status IN ('active', 'acknowledged')
      ORDER BY timestamp DESC 
      LIMIT ?
    `,
      [limit]
    );

    return rows.map(this.mapRowToAlert);
  }

  async getAlertHistory(filters: AlertFilters = {}): Promise<Alert[]> {
    let query = 'SELECT * FROM alerts WHERE 1=1';
    const values: any[] = [];

    if (filters.severity) {
      query += ' AND severity = ?';
      values.push(filters.severity);
    }

    if (filters.status) {
      query += ' AND status = ?';
      values.push(filters.status);
    }

    if (filters.startDate) {
      query += ' AND timestamp >= ?';
      values.push(filters.startDate);
    }

    if (filters.endDate) {
      query += ' AND timestamp <= ?';
      values.push(filters.endDate);
    }

    query += ' ORDER BY timestamp DESC';

    if (filters.limit) {
      query += ' LIMIT ?';
      values.push(filters.limit);
    }

    const rows = await this.db.query(query, values);
    return rows.map(this.mapRowToAlert);
  }

  private mapRowToAlert(row: any): Alert {
    return {
      id: row.id,
      ruleId: row.rule_id,
      severity: row.severity,
      title: row.title,
      description: row.description,
      correlationId: row.correlation_id,
      timestamp: new Date(row.timestamp),
      metadata: JSON.parse(row.metadata),
      status: row.status,
      acknowledgedBy: row.acknowledged_by,
      acknowledgedAt: row.acknowledged_at ? new Date(row.acknowledged_at) : undefined,
      resolvedBy: row.resolved_by,
      resolvedAt: row.resolved_at ? new Date(row.resolved_at) : undefined,
      resolution: row.resolution,
      deliveryResults: JSON.parse(row.delivery_results),
    };
  }
}
```

## Testing Strategy

### 1. Unit Tests

```typescript
// packages/observability/test/alert-manager.test.ts
describe('TammaAlertManager', () => {
  let alertManager: TammaAlertManager;

  beforeEach(() => {
    alertManager = new TammaAlertManager();
  });

  test('evaluates alert condition correctly', async () => {
    const metric: MetricData = {
      name: 'test_metric',
      value: 10,
      labels: {},
    };

    const rule: AlertRule = {
      id: 'test-rule',
      name: 'Test Rule',
      description: 'Test rule',
      enabled: true,
      severity: 'warning',
      condition: {
        metric: 'test_metric',
        operator: 'gt',
        threshold: 5,
      },
      cooldown: 0,
      channels: [],
      metadata: {},
    };

    const alert = await alertManager.evaluateAlert(metric, rule);

    expect(alert).toBeTruthy();
    expect(alert!.severity).toBe('warning');
    expect(alert!.title).toContain('Test Rule');
  });

  test('respects rate limiting', async () => {
    const metric: MetricData = {
      name: 'test_metric',
      value: 10,
      labels: {},
    };

    const rule: AlertRule = {
      id: 'test-rule',
      name: 'Test Rule',
      description: 'Test rule',
      enabled: true,
      severity: 'warning',
      condition: {
        metric: 'test_metric',
        operator: 'gt',
        threshold: 5,
      },
      cooldown: 0,
      channels: [],
      metadata: {},
    };

    // First alert should be created
    const alert1 = await alertManager.evaluateAlert(metric, rule);
    expect(alert1).toBeTruthy();

    // Next 4 alerts should be created (rate limit is 5 per minute)
    for (let i = 0; i < 4; i++) {
      const alert = await alertManager.evaluateAlert(metric, rule);
      expect(alert).toBeTruthy();
    }

    // 6th alert should be rate limited
    const alert6 = await alertManager.evaluateAlert(metric, rule);
    expect(alert6).toBeFalsy();
  });

  test('acknowledges and resolves alerts correctly', async () => {
    const alert: Alert = {
      id: 'test-alert',
      ruleId: 'test-rule',
      severity: 'warning',
      title: 'Test Alert',
      description: 'Test description',
      timestamp: new Date(),
      metadata: {},
      status: 'active',
      deliveryResults: [],
    };

    // Add alert to active alerts
    alertManager['activeAlerts'].set(alert.id, alert);

    // Acknowledge alert
    await alertManager.acknowledgeAlert(alert.id, 'test-user');

    const acknowledgedAlert = alertManager['activeAlerts'].get(alert.id);
    expect(acknowledgedAlert?.status).toBe('acknowledged');
    expect(acknowledgedAlert?.acknowledgedBy).toBe('test-user');

    // Resolve alert
    await alertManager.resolveAlert(alert.id, 'test-user', 'Fixed the issue');

    const resolvedAlert = alertManager['activeAlerts'].get(alert.id);
    expect(resolvedAlert).toBeUndefined(); // Should be removed from active alerts
  });
});
```

### 2. Integration Tests

```typescript
// packages/observability/test/alert-integration.test.ts
describe('Alert Integration', () => {
  let alertManager: TammaAlertManager;
  let metrics: PrometheusMetricsCollector;
  let integration: AlertIntegration;

  beforeEach(() => {
    alertManager = new TammaAlertManager();
    metrics = new PrometheusMetricsCollector();
    integration = new AlertIntegration(alertManager, metrics);
  });

  test('triggers alerts based on metrics', async () => {
    const metric: MetricData = {
      name: 'escalations_total',
      value: 3,
      labels: { reason: 'retry_exhausted' },
    };

    // Emit metric
    metrics.emit('metric', metric);

    // Wait for alert evaluation
    await new Promise((resolve) => setTimeout(resolve, 100));

    const activeAlerts = await alertManager.getActiveAlerts();
    expect(activeAlerts.length).toBeGreaterThan(0);

    const escalationAlert = activeAlerts.find((alert) => alert.ruleId === 'escalation-retries');
    expect(escalationAlert).toBeTruthy();
    expect(escalationAlert!.severity).toBe('critical');
  });

  test('delivers alerts to configured channels', async () => {
    const testWebhook = jest.fn();

    // Mock webhook delivery
    global.fetch = jest.fn().mockResolvedValue({
      ok: true,
      status: 200,
    });

    const alert: Alert = {
      id: 'test-alert',
      ruleId: 'test-rule',
      severity: 'warning',
      title: 'Test Alert',
      description: 'Test description',
      timestamp: new Date(),
      metadata: {},
      status: 'active',
      deliveryResults: [],
    };

    const results = await alertManager.sendAlert(alert);

    expect(results.length).toBeGreaterThan(0);
    expect(results[0].status).toBe('success');
  });
});
```

## Success Metrics

### Alert Effectiveness

- [ ] Critical alerts detected within 5 seconds
- [ ] False positive rate < 5%
- [ ] Alert delivery success rate > 95%
- [ ] Mean time to acknowledge < 10 minutes
- [ ] Mean time to resolve < 1 hour

### System Performance

- [ ] Alert evaluation overhead < 2ms
- [ ] Memory usage < 50MB for alert storage
- [ ] Rate limiting prevents alert spam
- [ ] Cooldown periods reduce noise

### Operational Excellence

- [ ] All critical conditions monitored
- [ ] Alert history retained for 30 days
- [ ] Alert acknowledgment workflow functional
- [ ] Integration with monitoring systems
- [ ] Alert testing and validation working

## Dependencies

### Core Dependencies

```json
{
  "nodemailer": "^6.9.7",
  "@slack/web-api": "^6.9.0"
}
```

### Development Dependencies

```json
{
  "@types/nodemailer": "^6.4.14",
  "nock": "^13.3.8"
}
```

## Risks and Mitigations

### Alert Fatigue

- **Risk**: Too many alerts reduce effectiveness
- **Mitigation**: Rate limiting, cooldown periods, severity classification

### Delivery Failures

- **Risk**: Alerts not delivered due to network issues
- **Mitigation**: Retry mechanisms, multiple channels, delivery tracking

### Performance Impact

- **Risk**: Alert evaluation affects system performance
- **Mitigation**: Efficient evaluation, async processing, performance monitoring

### Configuration Errors

- **Risk**: Misconfigured rules cause missed alerts
- **Mitigation**: Rule validation, testing framework, monitoring

## Rollout Plan

### Phase 1: Core Alert System (Week 1)

1. Alert manager implementation
2. Default alert rules
3. CLI and webhook channels

### Phase 2: Integration (Week 2)

1. Metrics integration
2. Alert storage
3. API endpoints

### Phase 3: Advanced Features (Week 3)

1. Email and Slack integration
2. Alert acknowledgment workflow
3. Alert history and reporting

### Phase 4: Testing and Deployment (Week 4)

1. Comprehensive testing
2. Performance optimization
3. Production deployment
4. Documentation and training

## Completion Criteria

- [ ] Alert system implemented with core functionality
- [ ] Default alert rules for critical conditions
- [ ] Multiple alert channels (CLI, webhook, email, Slack)
- [ ] Rate limiting and cooldown periods working
- [ ] Alert acknowledgment and resolution workflow
- [ ] Alert history storage and retrieval
- [ ] API endpoints for alert management
- [ ] Integration with metrics system
- [ ] Performance overhead under 2ms
- [ ] Comprehensive test coverage
- [ ] Documentation completed
- [ ] Production deployment successful

---

**Story Context**: This story implements a comprehensive alert system that monitors critical system conditions and notifies operators through multiple channels, ensuring rapid response to escalations, system errors, API rate limits, and event store failures while preventing alert fatigue through intelligent rate limiting and cooldown mechanisms.
