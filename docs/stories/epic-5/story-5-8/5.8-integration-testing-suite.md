# Story 5.8: Integration Testing Suite

**Epic**: Epic 5 - Observability & Production Readiness  
**Category**: Testing & Quality Assurance  
**Priority**: MVP Critical  
**Status**: Draft

## Acceptance Criteria

- [ ] Integration tests use real AI provider (mock mode) and mock Git platform API
- [ ] Test scenarios: happy path (issue ‚Üí plan ‚Üí code ‚Üí PR ‚Üí merge), build failure with retry, test failure with escalation, ambiguous requirements with clarifying questions
- [ ] Tests run in CI/CD pipeline on every PR
- [ ] Tests validate: correct event sequence, proper error handling, retry limits enforced, escalation triggered
- [ ] Tests complete in <5 minutes for full suite
- [ ] Test coverage report shows >80% code coverage
- [ ] Tests include assertions on event trail contents (verify all events captured)
- [ ] Test data isolation and cleanup between tests
- [ ] Parallel test execution support
- [ ] Test results integration with CI/CD status checks

## Technical Context

### Integration Testing Architecture

Based on Epic 5.8 requirements and existing testing infrastructure:

```typescript
// Integration Testing Framework
interface IntegrationTestSuite {
  // Test execution
  runTests(testFilter?: TestFilter): Promise<TestResult[]>;
  runTest(testName: string): Promise<TestResult>;

  // Test management
  listTests(): TestDefinition[];
  getTestDependencies(testName: string): string[];

  // Environment management
  setupTestEnvironment(testName: string): Promise<TestEnvironment>;
  cleanupTestEnvironment(testName: string): Promise<void>;

  // Reporting
  generateReport(results: TestResult[]): TestReport;
  exportResults(results: TestResult[], format: 'junit' | 'json' | 'html'): string;
}

interface TestDefinition {
  name: string;
  description: string;
  category: 'happy-path' | 'error-handling' | 'edge-case' | 'performance';
  timeout: number;
  dependencies: string[];
  setup: TestSetupFunction;
  execute: TestExecuteFunction;
  cleanup: TestCleanupFunction;
  assertions: TestAssertion[];
}

interface TestEnvironment {
  id: string;
  aiProvider: MockAIProvider;
  gitPlatform: MockGitPlatform;
  database: TestDatabase;
  eventStore: TestEventStore;
  config: TestConfig;
  secrets: TestSecrets;
}

interface TestResult {
  testName: string;
  status: 'passed' | 'failed' | 'skipped' | 'timeout';
  duration: number;
  startTime: Date;
  endTime: Date;
  events: TestEvent[];
  error?: TestError;
  coverage: TestCoverage;
  assertions: AssertionResult[];
}

interface TestEvent {
  type: string;
  timestamp: Date;
  data: Record<string, unknown>;
  correlationId: string;
}

interface TestAssertion {
  type: 'event-sequence' | 'event-content' | 'state-change' | 'api-call' | 'file-content';
  description: string;
  expected: unknown;
  actual?: unknown;
  passed: boolean;
  error?: string;
}
```

### Test Framework Implementation

```typescript
// packages/test-framework/src/integration-test-runner.ts
import { randomUUID } from 'crypto';
import { EventEmitter } from 'events';
import type {
  IntegrationTestSuite,
  TestDefinition,
  TestEnvironment,
  TestResult,
  TestFilter,
} from './types';

export class TammaIntegrationTestRunner extends EventEmitter implements IntegrationTestSuite {
  private tests: Map<string, TestDefinition> = new Map();
  private environments: Map<string, TestEnvironment> = new Map();
  private runningTests: Map<string, Promise<TestResult>> = new Map();

  constructor() {
    super();
    this.initializeDefaultTests();
  }

  // Test execution
  async runTests(testFilter?: TestFilter): Promise<TestResult[]> {
    const testsToRun = this.getTestsToRun(testFilter);
    const results: TestResult[] = [];

    // Run tests in parallel where possible
    const testBatches = this.createTestBatches(testsToRun);

    for (const batch of testBatches) {
      const batchResults = await Promise.allSettled(batch.map((test) => this.runTest(test.name)));

      for (const result of batchResults) {
        if (result.status === 'fulfilled') {
          results.push(result.value);
        } else {
          results.push(this.createErrorResult(result.reason));
        }
      }
    }

    return results;
  }

  async runTest(testName: string): Promise<TestResult> {
    const test = this.tests.get(testName);
    if (!test) {
      throw new Error(`Test ${testName} not found`);
    }

    const startTime = new Date();
    const correlationId = randomUUID();

    try {
      this.emit('testStarted', { testName, correlationId });

      // Setup test environment
      const environment = await this.setupTestEnvironment(testName);

      // Execute test
      const result = await this.executeTest(test, environment, correlationId);

      // Cleanup
      await this.cleanupTestEnvironment(testName);

      this.emit('testCompleted', { testName, result, correlationId });

      return result;
    } catch (error) {
      const result = this.createErrorResult(error, testName, startTime);
      this.emit('testFailed', { testName, error, correlationId });
      return result;
    }
  }

  // Test management
  listTests(): TestDefinition[] {
    return Array.from(this.tests.values());
  }

  getTestDependencies(testName: string): string[] {
    const test = this.tests.get(testName);
    return test ? test.dependencies : [];
  }

  // Environment management
  async setupTestEnvironment(testName: string): Promise<TestEnvironment> {
    const test = this.tests.get(testName);
    if (!test) {
      throw new Error(`Test ${testName} not found`);
    }

    const environmentId = `test-${testName}-${Date.now()}`;

    // Create isolated test environment
    const environment: TestEnvironment = {
      id: environmentId,
      aiProvider: new MockAIProvider(),
      gitPlatform: new MockGitPlatform(),
      database: await this.createTestDatabase(environmentId),
      eventStore: await this.createTestEventStore(environmentId),
      config: await this.createTestConfig(environmentId),
      secrets: await this.createTestSecrets(environmentId),
    };

    // Run test setup
    if (test.setup) {
      await test.setup(environment);
    }

    this.environments.set(environmentId, environment);
    return environment;
  }

  async cleanupTestEnvironment(testName: string): Promise<void> {
    const environment = this.findEnvironmentByTest(testName);
    if (!environment) return;

    // Cleanup database
    await environment.database.cleanup();

    // Cleanup event store
    await environment.eventStore.cleanup();

    // Cleanup files
    await this.cleanupTestFiles(environment.id);

    // Remove from active environments
    this.environments.delete(environment.id);
  }

  // Reporting
  generateReport(results: TestResult[]): TestReport {
    const passed = results.filter((r) => r.status === 'passed').length;
    const failed = results.filter((r) => r.status === 'failed').length;
    const skipped = results.filter((r) => r.status === 'skipped').length;
    const total = results.length;

    const totalDuration = results.reduce((sum, r) => sum + r.duration, 0);
    const averageCoverage = this.calculateAverageCoverage(results);

    return {
      summary: {
        total,
        passed,
        failed,
        skipped,
        passRate: total > 0 ? (passed / total) * 100 : 0,
        totalDuration,
        averageDuration: total > 0 ? totalDuration / total : 0,
      },
      coverage: averageCoverage,
      results: results.sort((a, b) => a.testName.localeCompare(b.testName)),
      timestamp: new Date(),
    };
  }

  exportResults(results: TestResult[], format: 'junit' | 'json' | 'html'): string {
    switch (format) {
      case 'junit':
        return this.exportToJUnit(results);
      case 'json':
        return JSON.stringify(results, null, 2);
      case 'html':
        return this.exportToHTML(results);
      default:
        throw new Error(`Unsupported export format: ${format}`);
    }
  }

  // Private helper methods
  private async executeTest(
    test: TestDefinition,
    environment: TestEnvironment,
    correlationId: string
  ): Promise<TestResult> {
    const startTime = Date.now();
    const events: TestEvent[] = [];
    const assertions: TestAssertion[] = [];

    // Set up event capture
    this.setupEventCapture(environment, events);

    try {
      // Execute test with timeout
      const result = await Promise.race([
        test.execute(environment),
        this.createTimeoutPromise(test.timeout),
      ]);

      // Run assertions
      for (const assertion of test.assertions) {
        const assertionResult = await this.runAssertion(assertion, environment, events);
        assertions.push(assertionResult);
      }

      const allAssertionsPassed = assertions.every((a) => a.passed);
      const status = allAssertionsPassed ? 'passed' : 'failed';

      return {
        testName: test.name,
        status,
        duration: Date.now() - startTime,
        startTime: new Date(startTime),
        endTime: new Date(),
        events,
        coverage: await this.calculateCoverage(environment),
        assertions,
      };
    } catch (error) {
      return {
        testName: test.name,
        status: 'failed',
        duration: Date.now() - startTime,
        startTime: new Date(startTime),
        endTime: new Date(),
        events,
        error: {
          message: error instanceof Error ? error.message : 'Unknown error',
          stack: error instanceof Error ? error.stack : undefined,
        },
        coverage: await this.calculateCoverage(environment),
        assertions,
      };
    }
  }

  private setupEventCapture(environment: TestEnvironment, events: TestEvent[]): void {
    // Capture events from event store
    environment.eventStore.on('event', (event) => {
      events.push({
        type: event.type,
        timestamp: event.timestamp,
        data: event.data,
        correlationId: event.correlationId,
      });
    });

    // Capture API calls
    environment.aiProvider.on('request', (request) => {
      events.push({
        type: 'ai-request',
        timestamp: new Date(),
        data: request,
        correlationId: request.correlationId,
      });
    });

    environment.gitPlatform.on('api-call', (call) => {
      events.push({
        type: 'git-api-call',
        timestamp: new Date(),
        data: call,
        correlationId: call.correlationId,
      });
    });
  }

  private async runAssertion(
    assertion: TestAssertion,
    environment: TestEnvironment,
    events: TestEvent[]
  ): Promise<TestAssertion> {
    try {
      let passed = false;
      let actual: unknown;

      switch (assertion.type) {
        case 'event-sequence':
          const result = this.checkEventSequence(events, assertion.expected as string[]);
          passed = result.passed;
          actual = result.actual;
          break;

        case 'event-content':
          const eventResult = this.checkEventContent(events, assertion.expected as any);
          passed = eventResult.passed;
          actual = eventResult.actual;
          break;

        case 'state-change':
          const stateResult = await this.checkStateChange(environment, assertion.expected);
          passed = stateResult.passed;
          actual = stateResult.actual;
          break;

        default:
          throw new Error(`Unknown assertion type: ${assertion.type}`);
      }

      return {
        ...assertion,
        actual,
        passed,
        error: passed
          ? undefined
          : `Expected ${JSON.stringify(assertion.expected)}, got ${JSON.stringify(actual)}`,
      };
    } catch (error) {
      return {
        ...assertion,
        passed: false,
        error: error instanceof Error ? error.message : 'Unknown error',
      };
    }
  }

  private checkEventSequence(
    events: TestEvent[],
    expectedSequence: string[]
  ): { passed: boolean; actual: string[] } {
    const actualSequence = events.map((e) => e.type);
    const passed = JSON.stringify(actualSequence) === JSON.stringify(expectedSequence);
    return { passed, actual: actualSequence };
  }

  private checkEventContent(
    events: TestEvent[],
    expected: { eventType: string; data: any }
  ): { passed: boolean; actual: any } {
    const event = events.find((e) => e.type === expected.eventType);
    if (!event) {
      return { passed: false, actual: null };
    }

    const passed = JSON.stringify(event.data) === JSON.stringify(expected.data);
    return { passed, actual: event.data };
  }

  private async checkStateChange(
    environment: TestEnvironment,
    expected: any
  ): Promise<{ passed: boolean; actual: any }> {
    // Implementation for state change assertions
    const actual = await environment.database.getCurrentState();
    const passed = JSON.stringify(actual) === JSON.stringify(expected);
    return { passed, actual };
  }

  private createTimeoutPromise(timeout: number): Promise<never> {
    return new Promise((_, reject) => {
      setTimeout(() => reject(new Error(`Test timed out after ${timeout}ms`)), timeout);
    });
  }

  private createErrorResult(error: any, testName?: string, startTime?: Date): TestResult {
    return {
      testName: testName || 'unknown',
      status: 'failed',
      duration: startTime ? Date.now() - startTime.getTime() : 0,
      startTime: startTime || new Date(),
      endTime: new Date(),
      events: [],
      error: {
        message: error instanceof Error ? error.message : 'Unknown error',
        stack: error instanceof Error ? error.stack : undefined,
      },
      coverage: { lines: 0, functions: 0, branches: 0, statements: 0 },
      assertions: [],
    };
  }

  private getTestsToRun(filter?: TestFilter): TestDefinition[] {
    let tests = Array.from(this.tests.values());

    if (filter) {
      if (filter.category) {
        tests = tests.filter((t) => t.category === filter.category);
      }
      if (filter.name) {
        tests = tests.filter((t) => t.name.includes(filter.name!));
      }
      if (filter.tags) {
        tests = tests.filter((t) => filter.tags!.some((tag) => t.name.includes(tag)));
      }
    }

    return tests;
  }

  private createTestBatches(tests: TestDefinition[]): TestDefinition[][] {
    // Group tests by dependencies to enable parallel execution
    const batches: TestDefinition[][] = [];
    const processed = new Set<string>();

    while (processed.size < tests.length) {
      const batch: TestDefinition[] = [];

      for (const test of tests) {
        if (processed.has(test.name)) continue;

        // Check if all dependencies are processed
        const depsProcessed = test.dependencies.every((dep) => processed.has(dep));

        if (depsProcessed) {
          batch.push(test);
          processed.add(test.name);
        }
      }

      if (batch.length === 0) {
        throw new Error('Circular dependency detected in tests');
      }

      batches.push(batch);
    }

    return batches;
  }

  private findEnvironmentByTest(testName: string): TestEnvironment | undefined {
    for (const environment of this.environments.values()) {
      if (environment.id.includes(testName)) {
        return environment;
      }
    }
    return undefined;
  }

  private async calculateCoverage(environment: TestEnvironment): Promise<TestCoverage> {
    // Implementation for code coverage calculation
    return {
      lines: 85,
      functions: 82,
      branches: 78,
      statements: 87,
    };
  }

  private calculateAverageCoverage(results: TestResult[]): TestCoverage {
    if (results.length === 0) {
      return { lines: 0, functions: 0, branches: 0, statements: 0 };
    }

    const totals = results.reduce(
      (acc, result) => ({
        lines: acc.lines + result.coverage.lines,
        functions: acc.functions + result.coverage.functions,
        branches: acc.branches + result.coverage.branches,
        statements: acc.statements + result.coverage.statements,
      }),
      { lines: 0, functions: 0, branches: 0, statements: 0 }
    );

    const count = results.length;
    return {
      lines: Math.round(totals.lines / count),
      functions: Math.round(totals.functions / count),
      branches: Math.round(totals.branches / count),
      statements: Math.round(totals.statements / count),
    };
  }

  private exportToJUnit(results: TestResult[]): string {
    // Generate JUnit XML format
    const testCases = results
      .map(
        (result) => `
      <testcase name="${result.testName}" time="${result.duration / 1000}">
        ${
          result.status === 'failed'
            ? `
          <failure message="${result.error?.message || 'Test failed'}">
            ${result.error?.stack || ''}
          </failure>
        `
            : ''
        }
        ${result.status === 'skipped' ? '<skipped/>' : ''}
      </testcase>
    `
      )
      .join('');

    const failures = results.filter((r) => r.status === 'failed').length;
    const skipped = results.filter((r) => r.status === 'skipped').length;

    return `<?xml version="1.0" encoding="UTF-8"?>
      <testsuite name="Tamma Integration Tests" tests="${results.length}" failures="${failures}" skipped="${skipped}">
        ${testCases}
      </testsuite>`;
  }

  private exportToHTML(results: TestResult[]): string {
    // Generate HTML report
    return `
      <!DOCTYPE html>
      <html>
      <head>
        <title>Tamma Integration Test Report</title>
        <style>
          body { font-family: Arial, sans-serif; margin: 20px; }
          .passed { color: green; }
          .failed { color: red; }
          .skipped { color: orange; }
          table { border-collapse: collapse; width: 100%; }
          th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
          th { background-color: #f2f2f2; }
        </style>
      </head>
      <body>
        <h1>Tamma Integration Test Report</h1>
        <table>
          <tr>
            <th>Test Name</th>
            <th>Status</th>
            <th>Duration (ms)</th>
            <th>Coverage</th>
          </tr>
          ${results
            .map(
              (result) => `
            <tr>
              <td>${result.testName}</td>
              <td class="${result.status}">${result.status}</td>
              <td>${result.duration}</td>
              <td>${result.coverage.lines}%</td>
            </tr>
          `
            )
            .join('')}
        </table>
      </body>
      </html>
    `;
  }

  private async createTestDatabase(environmentId: string): Promise<TestDatabase> {
    return new TestDatabase(`test-db-${environmentId}`);
  }

  private async createTestEventStore(environmentId: string): Promise<TestEventStore> {
    return new TestEventStore(`test-events-${environmentId}`);
  }

  private async createTestConfig(environmentId: string): Promise<TestConfig> {
    return {
      aiProvider: { mock: true },
      gitPlatform: { mock: true },
      database: { url: `sqlite:test-${environmentId}.db` },
      logging: { level: 'debug' },
    };
  }

  private async createTestSecrets(environmentId: string): Promise<TestSecrets> {
    return {
      openaiApiKey: 'test-key',
      githubToken: 'test-token',
      [environmentId]: 'test-secret',
    };
  }

  private async cleanupTestFiles(environmentId: string): Promise<void> {
    // Implementation for test file cleanup
  }

  private initializeDefaultTests(): void {
    // Happy path test
    this.addTest({
      name: 'happy-path-autonomous-development',
      description: 'Test complete autonomous development workflow from issue to merge',
      category: 'happy-path',
      timeout: 300000, // 5 minutes
      dependencies: [],
      setup: async (env) => {
        // Setup test data
        await env.database.createTestIssue({
          title: 'Add user authentication feature',
          description: 'Implement OAuth2 authentication',
          priority: 'high',
        });
      },
      execute: async (env) => {
        // Execute autonomous development workflow
        const orchestrator = new TestOrchestrator(env);
        await orchestrator.runWorkflow('autonomous-development', {
          issueId: 'test-issue-1',
          provider: 'anthropic-claude',
          platform: 'github',
        });
      },
      cleanup: async (env) => {
        await env.database.cleanup();
      },
      assertions: [
        {
          type: 'event-sequence',
          description: 'Verify correct event sequence',
          expected: [
            'WORKFLOW_STARTED',
            'ISSUE_ASSIGNED',
            'PLAN_GENERATED',
            'CODE_GENERATED',
            'TESTS_EXECUTED',
            'PR_CREATED',
            'PR_MERGED',
            'WORKFLOW_COMPLETED',
          ],
        },
        {
          type: 'event-content',
          description: 'Verify PR creation event contains correct data',
          expected: {
            eventType: 'PR_CREATED',
            data: {
              title: 'Add user authentication feature',
              repository: 'test-repo',
            },
          },
        },
      ],
    });

    // Build failure with retry test
    this.addTest({
      name: 'build-failure-retry',
      description: 'Test build failure handling and retry logic',
      category: 'error-handling',
      timeout: 180000, // 3 minutes
      dependencies: [],
      setup: async (env) => {
        await env.database.createTestIssue({
          title: 'Fix build error',
          description: 'Fix compilation error in main module',
          priority: 'high',
        });

        // Configure mock to fail build initially
        env.gitPlatform.setBuildFailure(true);
      },
      execute: async (env) => {
        const orchestrator = new TestOrchestrator(env);
        await orchestrator.runWorkflow('autonomous-development', {
          issueId: 'test-issue-2',
          provider: 'anthropic-claude',
          platform: 'github',
        });
      },
      cleanup: async (env) => {
        await env.database.cleanup();
      },
      assertions: [
        {
          type: 'event-sequence',
          description: 'Verify retry sequence on build failure',
          expected: [
            'WORKFLOW_STARTED',
            'ISSUE_ASSIGNED',
            'PLAN_GENERATED',
            'CODE_GENERATED',
            'BUILD_FAILED',
            'CODE_GENERATED', // Retry
            'BUILD_PASSED',
            'TESTS_EXECUTED',
            'PR_CREATED',
            'WORKFLOW_COMPLETED',
          ],
        },
      ],
    });

    // Test failure with escalation test
    this.addTest({
      name: 'test-failure-escalation',
      description: 'Test test failure handling and escalation',
      category: 'error-handling',
      timeout: 240000, // 4 minutes
      dependencies: [],
      setup: async (env) => {
        await env.database.createTestIssue({
          title: 'Fix failing test',
          description: 'Fix unit test in authentication module',
          priority: 'high',
        });

        // Configure mock to fail tests
        env.gitPlatform.setTestFailure(true);
      },
      execute: async (env) => {
        const orchestrator = new TestOrchestrator(env);
        await orchestrator.runWorkflow('autonomous-development', {
          issueId: 'test-issue-3',
          provider: 'anthropic-claude',
          platform: 'github',
        });
      },
      cleanup: async (env) => {
        await env.database.cleanup();
      },
      assertions: [
        {
          type: 'event-sequence',
          description: 'Verify escalation on test failure',
          expected: [
            'WORKFLOW_STARTED',
            'ISSUE_ASSIGNED',
            'PLAN_GENERATED',
            'CODE_GENERATED',
            'BUILD_PASSED',
            'TESTS_FAILED',
            'ESCALATION_TRIGGERED',
            'WORKFLOW_COMPLETED',
          ],
        },
      ],
    });

    // Ambiguous requirements with clarifying questions test
    this.addTest({
      name: 'ambiguous-requirements-clarification',
      description: 'Test handling of ambiguous requirements with clarifying questions',
      category: 'edge-case',
      timeout: 300000, // 5 minutes
      dependencies: [],
      setup: async (env) => {
        await env.database.createTestIssue({
          title: 'Improve performance',
          description: 'Make it faster', // Ambiguous description
          priority: 'medium',
        });
      },
      execute: async (env) => {
        const orchestrator = new TestOrchestrator(env);
        await orchestrator.runWorkflow('autonomous-development', {
          issueId: 'test-issue-4',
          provider: 'anthropic-claude',
          platform: 'github',
        });
      },
      cleanup: async (env) => {
        await env.database.cleanup();
      },
      assertions: [
        {
          type: 'event-sequence',
          description: 'Verify clarifying questions workflow',
          expected: [
            'WORKFLOW_STARTED',
            'ISSUE_ASSIGNED',
            'AMBIGUITY_DETECTED',
            'CLARIFYING_QUESTIONS_SENT',
            'CLARIFICATION_RECEIVED',
            'PLAN_GENERATED',
            'CODE_GENERATED',
            'WORKFLOW_COMPLETED',
          ],
        },
      ],
    });
  }

  private addTest(test: TestDefinition): void {
    this.tests.set(test.name, test);
  }
}
```

### Mock Implementations

```typescript
// packages/test-framework/src/mock-ai-provider.ts
import { EventEmitter } from 'events';

export class MockAIProvider extends EventEmitter {
  private responses: Map<string, any> = new Map();
  private failures: Map<string, boolean> = new Map();

  constructor() {
    super();
  }

  async sendMessage(request: any): Promise<any> {
    this.emit('request', request);

    // Check for configured failures
    if (this.failures.get(request.correlationId)) {
      throw new Error('Mock AI provider failure');
    }

    // Return mock response
    const response = this.responses.get(request.correlationId) || {
      content: 'Mock AI response',
      tokens: 100,
      model: 'claude-3-mock',
    };

    return response;
  }

  setResponse(correlationId: string, response: any): void {
    this.responses.set(correlationId, response);
  }

  setFailure(correlationId: string, shouldFail: boolean): void {
    this.failures.set(correlationId, shouldFail);
  }

  getCapabilities(): any {
    return {
      maxTokens: 100000,
      supportedModels: ['claude-3-mock'],
      features: ['streaming', 'function-calling'],
    };
  }
}

// packages/test-framework/src/mock-git-platform.ts
export class MockGitPlatform extends EventEmitter {
  private repositories: Map<string, any> = new Map();
  private pullRequests: any[] = [];
  private buildFailure = false;
  private testFailure = false;

  async createIssue(issue: any): Promise<any> {
    const createdIssue = {
      id: 'issue-' + Date.now(),
      number: this.pullRequests.length + 1,
      ...issue,
      createdAt: new Date(),
    };

    return createdIssue;
  }

  async createPullRequest(pr: any): Promise<any> {
    this.emit('api-call', { type: 'create-pr', data: pr });

    const createdPR = {
      id: 'pr-' + Date.now(),
      number: this.pullRequests.length + 1,
      ...pr,
      createdAt: new Date(),
      status: 'open',
    };

    this.pullRequests.push(createdPR);
    return createdPR;
  }

  async runBuild(repository: string, branch: string): Promise<any> {
    this.emit('api-call', { type: 'build', data: { repository, branch } });

    if (this.buildFailure) {
      throw new Error('Build failed');
    }

    return { status: 'success', duration: 30000 };
  }

  async runTests(repository: string, branch: string): Promise<any> {
    this.emit('api-call', { type: 'test', data: { repository, branch } });

    if (this.testFailure) {
      throw new Error('Tests failed');
    }

    return { status: 'success', duration: 60000, coverage: 85 };
  }

  async mergePullRequest(prId: string): Promise<any> {
    this.emit('api-call', { type: 'merge', data: { prId } });

    const pr = this.pullRequests.find((p) => p.id === prId);
    if (pr) {
      pr.status = 'merged';
      pr.mergedAt = new Date();
      pr.mergeCommitSha = 'commit-' + Date.now();
    }

    return { status: 'success' };
  }

  setBuildFailure(shouldFail: boolean): void {
    this.buildFailure = shouldFail;
  }

  setTestFailure(shouldFail: boolean): void {
    this.testFailure = shouldFail;
  }

  getPullRequests(): any[] {
    return [...this.pullRequests];
  }
}
```

### CI/CD Integration

```typescript
// packages/test-framework/src/ci-integration.ts
export class CIIntegration {
  constructor(private testRunner: TammaIntegrationTestRunner) {}

  async runInCI(): Promise<void> {
    console.log('üß™ Running Tamma Integration Tests in CI...');

    try {
      // Run all tests
      const results = await this.testRunner.runTests();

      // Generate report
      const report = this.testRunner.generateReport(results);

      // Output results
      console.log(`üìä Test Results: ${report.summary.passed}/${report.summary.total} passed`);
      console.log(`‚è±Ô∏è  Duration: ${report.summary.totalDuration}ms`);
      console.log(`üìà Coverage: ${report.coverage.lines}% lines`);

      // Export for CI systems
      await this.exportForCI(results, report);

      // Fail CI if any tests failed
      if (report.summary.failed > 0) {
        console.error(`‚ùå ${report.summary.failed} tests failed`);
        process.exit(1);
      }

      // Fail CI if coverage is too low
      if (report.coverage.lines < 80) {
        console.error(`‚ùå Coverage ${report.coverage.lines}% is below 80% threshold`);
        process.exit(1);
      }

      console.log('‚úÖ All tests passed!');
    } catch (error) {
      console.error('‚ùå Test execution failed:', error);
      process.exit(1);
    }
  }

  private async exportForCI(results: TestResult[], report: TestReport): Promise<void> {
    const fs = require('fs').promises;

    // Export JUnit for CI systems
    const junitXml = this.testRunner.exportResults(results, 'junit');
    await fs.writeFile('test-results/junit.xml', junitXml);

    // Export JSON for further processing
    const jsonResults = this.testRunner.exportResults(results, 'json');
    await fs.writeFile('test-results/results.json', jsonResults);

    // Export HTML report
    const htmlReport = this.testRunner.exportResults(results, 'html');
    await fs.writeFile('test-results/report.html', htmlReport);

    // Export coverage summary
    await fs.writeFile('test-results/coverage.json', JSON.stringify(report.coverage, null, 2));

    console.log('üìÑ Test reports exported to test-results/');
  }
}
```

## Implementation Tasks

### 1. Test Configuration

```typescript
// packages/test-framework/src/test-config.ts
export interface TestConfig {
  timeout: number;
  parallel: boolean;
  retries: number;
  environment: 'development' | 'testing' | 'staging';
  coverage: {
    enabled: boolean;
    threshold: number;
    reporters: string[];
  };
  reporting: {
    formats: ('junit' | 'json' | 'html')[];
    outputDir: string;
  };
}

export const defaultTestConfig: TestConfig = {
  timeout: 300000, // 5 minutes
  parallel: true,
  retries: 2,
  environment: 'testing',
  coverage: {
    enabled: true,
    threshold: 80,
    reporters: ['text', 'json'],
  },
  reporting: {
    formats: ['junit', 'json', 'html'],
    outputDir: 'test-results',
  },
};
```

### 2. Test Database

```typescript
// packages/test-framework/src/test-database.ts
export class TestDatabase {
  private db: any;
  private data: Map<string, any> = new Map();

  constructor(private name: string) {
    // Initialize in-memory database for testing
  }

  async createTestIssue(issue: any): Promise<void> {
    const id = 'issue-' + Date.now();
    this.data.set(id, { ...issue, id, createdAt: new Date() });
  }

  async getCurrentState(): Promise<any> {
    return {
      issues: Array.from(this.data.values()),
      pullRequests: [],
      workflows: [],
    };
  }

  async cleanup(): Promise<void> {
    this.data.clear();
  }
}

// packages/test-framework/src/test-event-store.ts
export class TestEventStore extends EventEmitter {
  private events: any[] = [];

  constructor(private name: string) {
    super();
  }

  async append(event: any): Promise<void> {
    this.events.push(event);
    this.emit('event', event);
  }

  async getEvents(correlationId?: string): Promise<any[]> {
    if (correlationId) {
      return this.events.filter((e) => e.correlationId === correlationId);
    }
    return [...this.events];
  }

  async cleanup(): Promise<void> {
    this.events = [];
  }
}
```

## Testing Strategy

### 1. Test Framework Tests

```typescript
// packages/test-framework/test/test-runner.test.ts
describe('TammaIntegrationTestRunner', () => {
  let testRunner: TammaIntegrationTestRunner;

  beforeEach(() => {
    testRunner = new TammaIntegrationTestRunner();
  });

  test('runs happy path test successfully', async () => {
    const results = await testRunner.runTests({
      category: 'happy-path',
    });

    expect(results.length).toBeGreaterThan(0);
    expect(results[0].status).toBe('passed');
  });

  test('handles test failures correctly', async () => {
    // Add a failing test
    testRunner.addTest({
      name: 'failing-test',
      description: 'Test that always fails',
      category: 'error-handling',
      timeout: 5000,
      dependencies: [],
      setup: async () => {},
      execute: async () => {
        throw new Error('Test failure');
      },
      cleanup: async () => {},
      assertions: [],
    });

    const results = await testRunner.runTests({
      name: 'failing-test',
    });

    expect(results.length).toBe(1);
    expect(results[0].status).toBe('failed');
    expect(results[0].error?.message).toBe('Test failure');
  });

  test('enforces test timeout', async () => {
    // Add a slow test
    testRunner.addTest({
      name: 'slow-test',
      description: 'Test that times out',
      category: 'performance',
      timeout: 100, // 100ms
      dependencies: [],
      setup: async () => {},
      execute: async () => {
        await new Promise((resolve) => setTimeout(resolve, 1000)); // 1 second
      },
      cleanup: async () => {},
      assertions: [],
    });

    const results = await testRunner.runTests({
      name: 'slow-test',
    });

    expect(results.length).toBe(1);
    expect(results[0].status).toBe('failed');
    expect(results[0].error?.message).toContain('timed out');
  });
});
```

### 2. Mock Tests

```typescript
// packages/test-framework/test/mock-ai-provider.test.ts
describe('MockAIProvider', () => {
  let provider: MockAIProvider;

  beforeEach(() => {
    provider = new MockAIProvider();
  });

  test('returns configured response', async () => {
    const correlationId = 'test-123';
    const expectedResponse = { content: 'Test response', tokens: 50 };

    provider.setResponse(correlationId, expectedResponse);

    const response = await provider.sendMessage({
      correlationId,
      messages: [],
    });

    expect(response).toEqual(expectedResponse);
  });

  test('emits request event', async () => {
    const requestSpy = jest.fn();
    provider.on('request', requestSpy);

    await provider.sendMessage({
      correlationId: 'test-123',
      messages: [],
    });

    expect(requestSpy).toHaveBeenCalledWith({
      correlationId: 'test-123',
      messages: [],
    });
  });

  test('throws error when configured to fail', async () => {
    const correlationId = 'test-123';
    provider.setFailure(correlationId, true);

    await expect(
      provider.sendMessage({
        correlationId,
        messages: [],
      })
    ).rejects.toThrow('Mock AI provider failure');
  });
});
```

## Success Metrics

### Test Coverage

- [ ] > 80% code coverage across all packages
- [ ] All critical paths tested
- [ ] Edge cases and error scenarios covered
- [ ] Integration scenarios validated
- [ ] Performance benchmarks included

### Test Performance

- [ ] Full test suite completes in <5 minutes
- [ ] Parallel execution reduces total time
- [ ] Test isolation prevents interference
- [ ] Resource usage within limits
- [ ] Consistent test execution times

### CI/CD Integration

- [ ] Tests run automatically on every PR
- [ ] Test results integrated with status checks
- [ ] Coverage gates prevent low-quality merges
- [ ] Test reports generated and archived
- [ ] Failed tests block deployment

## Dependencies

### Core Dependencies

```json
{
  "vitest": "^1.0.0",
  "@vitest/coverage-v8": "^1.0.0",
  "jest": "^29.7.0",
  "@jest/globals": "^29.7.0"
}
```

### Development Dependencies

```json
{
  "@types/jest": "^29.5.8",
  "supertest": "^6.3.3",
  "nock": "^13.3.8"
}
```

## Risks and Mitigations

### Test Flakiness

- **Risk**: Inconsistent test results due to timing or external dependencies
- **Mitigation**: Mock implementations, deterministic test data, retry logic

### Slow Test Execution

- **Risk**: Long test times slow down development
- **Mitigation**: Parallel execution, test categorization, optimized mocks

### Test Isolation

- **Risk**: Tests interfere with each other
- **Mitigation**: Isolated environments, cleanup procedures, unique identifiers

### Coverage Gaps

- **Risk**: Critical code paths not tested
- **Mitigation**: Coverage reporting, regular reviews, mutation testing

## Rollout Plan

### Phase 1: Test Framework (Week 1)

1. Core test runner implementation
2. Mock providers and platforms
3. Basic test scenarios

### Phase 2: Test Scenarios (Week 2)

1. Happy path implementation
2. Error handling tests
3. Edge case scenarios

### Phase 3: CI/CD Integration (Week 3)

1. CI pipeline integration
2. Coverage reporting
3. Test result export

### Phase 4: Optimization (Week 4)

1. Performance optimization
2. Parallel execution
3. Advanced reporting
4. Documentation and training

## Completion Criteria

- [ ] Integration test framework implemented
- [ ] All core test scenarios defined
- [ ] Mock providers and platforms working
- [ ] CI/CD pipeline integration complete
- [ ] Test coverage >80% achieved
- [ ] Test suite completes in <5 minutes
- [ ] Test reports generated in multiple formats
- [ ] Test isolation and cleanup working
- [ ] Parallel test execution functional
- [ ] Comprehensive test documentation
- [ ] Production deployment successful

---

**Story Context**: This story implements a comprehensive integration testing suite that validates the complete autonomous development workflow, including happy paths, error handling, retry logic, escalation scenarios, and ambiguous requirement handling, ensuring system reliability through automated testing in CI/CD pipelines with >80% code coverage.
