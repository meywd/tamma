# Story 5.2: Metrics Collection Infrastructure

**Epic**: Epic 5 - Observability & Production Readiness  
**Category**: Metrics & Monitoring  
**Priority**: MVP Critical  
**Status**: Draft

## Acceptance Criteria

- [ ] Metrics library integrated (Prometheus client recommended)
- [ ] Counter metrics: `issues_processed_total`, `prs_created_total`, `prs_merged_total`, `escalations_total`
- [ ] Gauge metrics: `active_autonomous_loops`, `pending_approvals`, `queue_depth`
- [ ] Histogram metrics: `issue_completion_duration_seconds`, `ai_request_duration_seconds`, `test_execution_duration_seconds`
- [ ] Metrics exposed via HTTP endpoint: `GET /metrics` (Prometheus format)
- [ ] Metrics include labels: provider name, Git platform, issue type, outcome (success/failure)
- [ ] Metrics scraped by Prometheus (or pushed to metrics backend) every 15 seconds
- [ ] Performance impact: <1% overhead on application performance
- [ ] Metrics retention and aggregation configured
- [ ] Alerting rules based on metrics thresholds

## Technical Context

### Metrics Architecture

Based on Epic 5.2 requirements and existing observability patterns:

```typescript
// Metrics Collection Interface
interface MetricsCollector {
  // Counters - monotonically increasing values
  incrementCounter(name: string, labels?: Record<string, string>, value?: number): void;

  // Gauges - values that can go up or down
  setGauge(name: string, value: number, labels?: Record<string, string>): void;
  incrementGauge(name: string, labels?: Record<string, string>, value?: number): void;
  decrementGauge(name: string, labels?: Record<string, string>, value?: number): void;

  // Histograms - value distributions
  observeHistogram(name: string, value: number, labels?: Record<string, string>): void;

  // Timers - convenience for duration histograms
  startTimer(name: string, labels?: Record<string, string>): () => void;
}

interface MetricDefinition {
  name: string;
  type: 'counter' | 'gauge' | 'histogram' | 'summary';
  help: string;
  labels?: string[];
  buckets?: number[]; // For histograms
}

// Standard metric labels
interface MetricLabels {
  provider?: string; // 'anthropic-claude', 'openai-gpt4', etc.
  platform?: string; // 'github', 'gitlab', etc.
  issueType?: string; // 'feature', 'bug', 'enhancement'
  outcome?: string; // 'success', 'failure', 'escalation'
  workflowId?: string; // Workflow identifier
  stepId?: string; // Step identifier
  userId?: string; // User identifier
  repository?: string; // Repository name
  errorType?: string; // Type of error
  severity?: string; // 'low', 'medium', 'high', 'critical'
}
```

### Prometheus Metrics Implementation

```typescript
// packages/observability/src/metrics.ts
import { register, Counter, Gauge, Histogram, Summary } from 'prom-client';
import type { MetricsCollector, MetricLabels } from './types';

export class PrometheusMetricsCollector implements MetricsCollector {
  private counters: Map<string, Counter<string>> = new Map();
  private gauges: Map<string, Gauge<string>> = new Map();
  private histograms: Map<string, Histogram<string>> = new Map();
  private summaries: Map<string, Summary<string>> = new Map();

  constructor() {
    this.initializeDefaultMetrics();
  }

  // Counter methods
  incrementCounter(name: string, labels?: Record<string, string>, value = 1): void {
    let counter = this.counters.get(name);
    if (!counter) {
      throw new Error(`Counter '${name}' not registered`);
    }

    if (labels) {
      counter.inc(labels, value);
    } else {
      counter.inc(value);
    }
  }

  // Gauge methods
  setGauge(name: string, value: number, labels?: Record<string, string>): void {
    let gauge = this.gauges.get(name);
    if (!gauge) {
      throw new Error(`Gauge '${name}' not registered`);
    }

    if (labels) {
      gauge.set(labels, value);
    } else {
      gauge.set(value);
    }
  }

  incrementGauge(name: string, labels?: Record<string, string>, value = 1): void {
    let gauge = this.gauges.get(name);
    if (!gauge) {
      throw new Error(`Gauge '${name}' not registered`);
    }

    if (labels) {
      gauge.inc(labels, value);
    } else {
      gauge.inc(value);
    }
  }

  decrementGauge(name: string, labels?: Record<string, string>, value = 1): void {
    let gauge = this.gauges.get(name);
    if (!gauge) {
      throw new Error(`Gauge '${name}' not registered`);
    }

    if (labels) {
      gauge.dec(labels, value);
    } else {
      gauge.dec(value);
    }
  }

  // Histogram methods
  observeHistogram(name: string, value: number, labels?: Record<string, string>): void {
    let histogram = this.histograms.get(name);
    if (!histogram) {
      throw new Error(`Histogram '${name}' not registered`);
    }

    if (labels) {
      histogram.observe(labels, value);
    } else {
      histogram.observe(value);
    }
  }

  // Timer convenience method
  startTimer(name: string, labels?: Record<string, string>): () => void {
    const start = Date.now();

    return () => {
      const duration = (Date.now() - start) / 1000; // Convert to seconds
      this.observeHistogram(name, duration, labels);
    };
  }

  // Registration methods
  registerCounter(name: string, help: string, labelNames?: string[]): void {
    if (this.counters.has(name)) {
      throw new Error(`Counter '${name}' already registered`);
    }

    const counter = new Counter({
      name,
      help,
      labelNames: labelNames || [],
    });

    this.counters.set(name, counter);
    register.registerMetric(counter);
  }

  registerGauge(name: string, help: string, labelNames?: string[]): void {
    if (this.gauges.has(name)) {
      throw new Error(`Gauge '${name}' already registered`);
    }

    const gauge = new Gauge({
      name,
      help,
      labelNames: labelNames || [],
    });

    this.gauges.set(name, gauge);
    register.registerMetric(gauge);
  }

  registerHistogram(name: string, help: string, labelNames?: string[], buckets?: number[]): void {
    if (this.histograms.has(name)) {
      throw new Error(`Histogram '${name}' already registered`);
    }

    const histogram = new Histogram({
      name,
      help,
      labelNames: labelNames || [],
      buckets: buckets || [0.1, 0.5, 1, 2, 5, 10, 30, 60, 300, 600, 1800, 3600],
    });

    this.histograms.set(name, histogram);
    register.registerMetric(histogram);
  }

  // Get metrics for Prometheus scraping
  getMetrics(): Promise<string> {
    return register.metrics();
  }

  private initializeDefaultMetrics(): void {
    // Business metrics
    this.registerCounter('issues_processed_total', 'Total number of issues processed', [
      'provider',
      'platform',
      'outcome',
    ]);
    this.registerCounter('prs_created_total', 'Total number of pull requests created', [
      'platform',
      'repository',
    ]);
    this.registerCounter('prs_merged_total', 'Total number of pull requests merged', [
      'platform',
      'repository',
    ]);
    this.registerCounter('escalations_total', 'Total number of escalations', [
      'reason',
      'severity',
    ]);

    // System metrics
    this.registerGauge('active_autonomous_loops', 'Number of currently active autonomous loops', [
      'workflow_type',
    ]);
    this.registerGauge('pending_approvals', 'Number of pending human approvals', ['type']);
    this.registerGauge('queue_depth', 'Current depth of processing queues', ['queue_name']);

    // Performance metrics
    this.registerHistogram('issue_completion_duration_seconds', 'Time taken to complete issues', [
      'provider',
      'platform',
      'issue_type',
    ]);
    this.registerHistogram('ai_request_duration_seconds', 'Time taken for AI provider requests', [
      'provider',
      'model',
    ]);
    this.registerHistogram('test_execution_duration_seconds', 'Time taken for test execution', [
      'test_type',
      'framework',
    ]);
    this.registerHistogram(
      'workflow_step_duration_seconds',
      'Time taken for individual workflow steps',
      ['step_name', 'workflow_type']
    );

    // Error metrics
    this.registerCounter('errors_total', 'Total number of errors', [
      'error_type',
      'severity',
      'component',
    ]);
    this.registerCounter('api_requests_total', 'Total number of API requests', [
      'service',
      'method',
      'status_code',
    ]);
    this.registerHistogram('api_request_duration_seconds', 'Duration of API requests', [
      'service',
      'method',
    ]);
  }
}
```

### Workflow Metrics Integration

```typescript
// packages/orchestrator/src/workflow-metrics.ts
import { PrometheusMetricsCollector } from '@tamma/observability';
import type { WorkflowContext, WorkflowStep } from './types';

export class WorkflowMetrics {
  constructor(private metrics: PrometheusMetricsCollector) {}

  // Workflow lifecycle metrics
  recordWorkflowStart(context: WorkflowContext): void {
    this.metrics.incrementCounter('workflows_started_total', {
      workflow_type: context.workflowType,
      provider: context.provider,
      platform: context.platform,
    });

    this.metrics.incrementGauge('active_autonomous_loops', {
      workflow_type: context.workflowType,
    });
  }

  recordWorkflowComplete(context: WorkflowContext, duration: number): void {
    this.metrics.incrementCounter('workflows_completed_total', {
      workflow_type: context.workflowType,
      provider: context.provider,
      platform: context.platform,
      outcome: 'success',
    });

    this.metrics.observeHistogram('workflow_completion_duration_seconds', duration, {
      workflow_type: context.workflowType,
      provider: context.provider,
      platform: context.platform,
    });

    this.metrics.decrementGauge('active_autonomous_loops', {
      workflow_type: context.workflowType,
    });
  }

  recordWorkflowError(context: WorkflowContext, error: Error): void {
    this.metrics.incrementCounter('workflows_completed_total', {
      workflow_type: context.workflowType,
      provider: context.provider,
      platform: context.platform,
      outcome: 'failure',
    });

    this.metrics.incrementCounter('errors_total', {
      error_type: error.constructor.name,
      severity: 'high',
      component: 'workflow',
    });

    this.metrics.decrementGauge('active_autonomous_loops', {
      workflow_type: context.workflowType,
    });
  }

  // Step metrics
  recordStepStart(step: WorkflowStep, context: WorkflowContext): void {
    // Timer is started and will be completed when step finishes
    return this.metrics.startTimer('workflow_step_duration_seconds', {
      step_name: step.name,
      workflow_type: context.workflowType,
    });
  }

  recordStepComplete(step: WorkflowStep, context: WorkflowContext, duration: number): void {
    this.metrics.observeHistogram('workflow_step_duration_seconds', duration, {
      step_name: step.name,
      workflow_type: context.workflowType,
    });
  }

  recordStepError(step: WorkflowStep, context: WorkflowContext, error: Error): void {
    this.metrics.incrementCounter('workflow_step_errors_total', {
      step_name: step.name,
      workflow_type: context.workflowType,
      error_type: error.constructor.name,
    });
  }

  // Issue processing metrics
  recordIssueProcessed(
    context: WorkflowContext,
    outcome: 'success' | 'failure' | 'escalation'
  ): void {
    this.metrics.incrementCounter('issues_processed_total', {
      provider: context.provider,
      platform: context.platform,
      outcome,
    });
  }

  recordEscalation(reason: string, severity: string, context: WorkflowContext): void {
    this.metrics.incrementCounter('escalations_total', {
      reason,
      severity,
      workflow_type: context.workflowType,
    });
  }
}
```

### AI Provider Metrics

```typescript
// packages/providers/src/provider-metrics.ts
import { PrometheusMetricsCollector } from '@tamma/observability';
import type { AIProvider, MessageRequest, MessageChunk } from './types';

export class ProviderMetrics {
  constructor(private metrics: PrometheusMetricsCollector) {}

  recordRequestStart(providerName: string, request: MessageRequest): () => void {
    this.metrics.incrementCounter('ai_requests_total', {
      provider: providerName,
      model: request.model,
      operation: 'message',
    });

    return this.metrics.startTimer('ai_request_duration_seconds', {
      provider: providerName,
      model: request.model,
    });
  }

  recordRequestComplete(
    providerName: string,
    request: MessageRequest,
    chunks: MessageChunk[],
    duration: number
  ): void {
    const totalTokens = chunks.reduce((sum, chunk) => sum + (chunk.tokens || 0), 0);

    this.metrics.observeHistogram('ai_request_duration_seconds', duration, {
      provider: providerName,
      model: request.model,
    });

    this.metrics.incrementCounter(
      'ai_tokens_total',
      {
        provider: providerName,
        model: request.model,
        token_type: 'total',
      },
      totalTokens
    );

    this.metrics.incrementCounter('ai_requests_successful_total', {
      provider: providerName,
      model: request.model,
    });
  }

  recordRequestError(providerName: string, request: MessageRequest, error: Error): void {
    this.metrics.incrementCounter('ai_requests_failed_total', {
      provider: providerName,
      model: request.model,
      error_type: error.constructor.name,
    });

    this.metrics.incrementCounter('errors_total', {
      error_type: error.constructor.name,
      severity: 'medium',
      component: 'ai_provider',
    });
  }

  recordRateLimit(providerName: string, resetTime: Date): void {
    this.metrics.incrementCounter('ai_rate_limits_total', {
      provider: providerName,
    });

    // Set gauge for time until reset
    const timeUntilReset = Math.max(0, resetTime.getTime() - Date.now()) / 1000;
    this.metrics.setGauge('ai_rate_limit_reset_seconds', timeUntilReset, {
      provider: providerName,
    });
  }

  recordTokenUsage(
    providerName: string,
    model: string,
    inputTokens: number,
    outputTokens: number
  ): void {
    this.metrics.incrementCounter(
      'ai_tokens_total',
      {
        provider: providerName,
        model: model,
        token_type: 'input',
      },
      inputTokens
    );

    this.metrics.incrementCounter(
      'ai_tokens_total',
      {
        provider: providerName,
        model: model,
        token_type: 'output',
      },
      outputTokens
    );
  }
}
```

### Git Platform Metrics

```typescript
// packages/platforms/src/platform-metrics.ts
import { PrometheusMetricsCollector } from '@tamma/observability';
import type { GitPlatform, PullRequest, Issue } from './types';

export class PlatformMetrics {
  constructor(private metrics: PrometheusMetricsCollector) {}

  recordAPIRequest(
    platformName: string,
    endpoint: string,
    method: string,
    statusCode: number,
    duration: number
  ): void {
    this.metrics.incrementCounter('api_requests_total', {
      service: 'git_platform',
      platform: platformName,
      method,
      status_code: statusCode.toString(),
    });

    this.metrics.observeHistogram('api_request_duration_seconds', duration, {
      service: 'git_platform',
      platform: platformName,
      method,
    });
  }

  recordIssueCreated(platformName: string, issue: Issue): void {
    this.metrics.incrementCounter('issues_created_total', {
      platform: platformName,
      repository: issue.repository,
    });
  }

  recordPRCreated(platformName: string, pr: PullRequest): void {
    this.metrics.incrementCounter('prs_created_total', {
      platform: platformName,
      repository: pr.repository,
    });
  }

  recordPRMerged(platformName: string, pr: PullRequest): void {
    this.metrics.incrementCounter('prs_merged_total', {
      platform: platformName,
      repository: pr.repository,
    });

    // Calculate PR lifetime
    if (pr.createdAt && pr.mergedAt) {
      const lifetime = (pr.mergedAt.getTime() - pr.createdAt.getTime()) / (1000 * 60 * 60); // hours
      this.metrics.observeHistogram('pr_lifetime_hours', lifetime, {
        platform: platformName,
        repository: pr.repository,
      });
    }
  }

  recordRateLimit(platformName: string, resetTime: Date): void {
    this.metrics.incrementCounter('platform_rate_limits_total', {
      platform: platformName,
    });

    const timeUntilReset = Math.max(0, resetTime.getTime() - Date.now()) / 1000;
    this.metrics.setGauge('platform_rate_limit_reset_seconds', timeUntilReset, {
      platform: platformName,
    });
  }

  recordWebhookReceived(platformName: string, eventType: string): void {
    this.metrics.incrementCounter('webhooks_received_total', {
      platform: platformName,
      event_type: eventType,
    });
  }

  recordWebhookProcessed(platformName: string, eventType: string, success: boolean): void {
    this.metrics.incrementCounter('webhooks_processed_total', {
      platform: platformName,
      event_type: eventType,
      outcome: success ? 'success' : 'failure',
    });
  }
}
```

### Metrics HTTP Endpoint

```typescript
// packages/api/src/metrics-endpoint.ts
import { FastifyInstance, FastifyRequest, FastifyReply } from 'fastify';
import { PrometheusMetricsCollector } from '@tamma/observability';

export class MetricsEndpoint {
  constructor(private metrics: PrometheusMetricsCollector) {}

  registerRoutes(fastify: FastifyInstance): void {
    // Metrics endpoint for Prometheus scraping
    fastify.get(
      '/metrics',
      {
        schema: {
          description: 'Prometheus metrics endpoint',
          tags: ['monitoring'],
          response: {
            200: {
              type: 'string',
              description: 'Prometheus-formatted metrics',
            },
          },
        },
      },
      async (request: FastifyRequest, reply: FastifyReply) => {
        try {
          const metrics = await this.metrics.getMetrics();

          reply.type('text/plain; version=0.0.4; charset=utf-8');
          return metrics;
        } catch (error) {
          reply.code(500);
          return { error: 'Failed to generate metrics' };
        }
      }
    );

    // Health check endpoint
    fastify.get(
      '/health',
      {
        schema: {
          description: 'Health check endpoint',
          tags: ['monitoring'],
          response: {
            200: {
              type: 'object',
              properties: {
                status: { type: 'string' },
                timestamp: { type: 'string' },
                uptime: { type: 'number' },
              },
            },
          },
        },
      },
      async (request: FastifyRequest, reply: FastifyReply) => {
        const uptime = process.uptime();

        return {
          status: 'healthy',
          timestamp: new Date().toISOString(),
          uptime,
        };
      }
    );

    // Ready check endpoint
    fastify.get(
      '/ready',
      {
        schema: {
          description: 'Readiness check endpoint',
          tags: ['monitoring'],
        },
      },
      async (request: FastifyRequest, reply: FastifyReply) => {
        // Check if all critical services are ready
        const isReady = await this.checkReadiness();

        if (isReady) {
          reply.code(200);
          return { status: 'ready' };
        } else {
          reply.code(503);
          return { status: 'not ready' };
        }
      }
    );
  }

  private async checkReadiness(): Promise<boolean> {
    // Implement readiness checks
    // - Database connectivity
    // - AI provider availability
    // - Git platform connectivity
    // - Queue system status
    return true; // Simplified for now
  }
}
```

## Implementation Tasks

### 1. Metrics Configuration

```typescript
// packages/observability/src/metrics-config.ts
export interface MetricsConfig {
  enabled: boolean;
  endpoint: string;
  port: number;
  path: string;
  labels: Record<string, string>;
  buckets: {
    duration: number[];
    size: number[];
    throughput: number[];
  };
  retention: {
    metrics: number; // days
    samples: number; // per metric
  };
}

export const defaultMetricsConfig: MetricsConfig = {
  enabled: process.env.METRICS_ENABLED !== 'false',
  endpoint: process.env.METRICS_ENDPOINT || '0.0.0.0',
  port: parseInt(process.env.METRICS_PORT || '9090'),
  path: process.env.METRICS_PATH || '/metrics',
  labels: {
    service: process.env.SERVICE_NAME || 'tamma',
    version: process.env.APP_VERSION || 'unknown',
    environment: process.env.NODE_ENV || 'development',
  },
  buckets: {
    duration: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 30, 60, 300, 600, 1800, 3600],
    size: [1024, 4096, 16384, 65536, 262144, 1048576, 4194304, 16777216],
    throughput: [0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000],
  },
  retention: {
    metrics: 30, // 30 days
    samples: 10000, // per metric
  },
};
```

### 2. Metrics Aggregation

```typescript
// packages/observability/src/aggregation.ts
export class MetricsAggregator {
  private aggregationRules: AggregationRule[] = [];

  constructor() {
    this.initializeDefaultRules();
  }

  addRule(rule: AggregationRule): void {
    this.aggregationRules.push(rule);
  }

  aggregate(metrics: MetricData[]): AggregatedMetrics[] {
    const aggregated: AggregatedMetrics[] = [];

    for (const rule of this.aggregationRules) {
      const matchingMetrics = metrics.filter((m) => rule.matches(m));
      const aggregatedValue = rule.aggregate(matchingMetrics);

      aggregated.push({
        name: rule.outputName,
        value: aggregatedValue,
        labels: rule.outputLabels,
        timestamp: new Date(),
      });
    }

    return aggregated;
  }

  private initializeDefaultRules(): void {
    // Success rate calculation
    this.addRule({
      name: 'success_rate',
      matches: (metric) => metric.name === 'issues_processed_total',
      aggregate: (metrics) => {
        const successful = metrics
          .filter((m) => m.labels.outcome === 'success')
          .reduce((sum, m) => sum + m.value, 0);
        const total = metrics.reduce((sum, m) => sum + m.value, 0);
        return total > 0 ? successful / total : 0;
      },
      outputName: 'issues_success_rate',
      outputLabels: {},
    });

    // Average duration calculation
    this.addRule({
      name: 'average_duration',
      matches: (metric) => metric.name === 'issue_completion_duration_seconds',
      aggregate: (metrics) => {
        const sum = metrics.reduce((sum, m) => sum + m.value, 0);
        return metrics.length > 0 ? sum / metrics.length : 0;
      },
      outputName: 'issues_average_completion_duration_seconds',
      outputLabels: {},
    });
  }
}

interface AggregationRule {
  name: string;
  matches: (metric: MetricData) => boolean;
  aggregate: (metrics: MetricData[]) => number;
  outputName: string;
  outputLabels: Record<string, string>;
}

interface MetricData {
  name: string;
  value: number;
  labels: Record<string, string>;
  timestamp: Date;
}

interface AggregatedMetrics {
  name: string;
  value: number;
  labels: Record<string, string>;
  timestamp: Date;
}
```

## Testing Strategy

### 1. Unit Tests

```typescript
// packages/observability/test/metrics.test.ts
describe('PrometheusMetricsCollector', () => {
  let metrics: PrometheusMetricsCollector;

  beforeEach(() => {
    metrics = new PrometheusMetricsCollector();
  });

  test('increments counter correctly', () => {
    metrics.incrementCounter('test_counter', { label: 'value' });

    const output = await metrics.getMetrics();
    expect(output).toContain('test_counter{label="value"} 1');
  });

  test('sets gauge correctly', () => {
    metrics.setGauge('test_gauge', 42, { label: 'value' });

    const output = await metrics.getMetrics();
    expect(output).toContain('test_gauge{label="value"} 42');
  });

  test('observes histogram correctly', () => {
    metrics.observeHistogram('test_histogram', 1.5, { label: 'value' });

    const output = await metrics.getMetrics();
    expect(output).toContain('test_histogram_bucket{label="value",le="+Inf"} 1');
  });

  test('timer measures duration correctly', async () => {
    const stopTimer = metrics.startTimer('test_timer');

    await new Promise((resolve) => setTimeout(resolve, 100));
    stopTimer();

    const output = await metrics.getMetrics();
    expect(output).toContain('test_timer_sum');
  });
});
```

### 2. Integration Tests

```typescript
// packages/observability/test/integration.test.ts
describe('Metrics Integration', () => {
  test('metrics endpoint returns valid Prometheus format', async () => {
    const metrics = new PrometheusMetricsCollector();
    const endpoint = new MetricsEndpoint(metrics);

    const app = fastify();
    endpoint.registerRoutes(app);

    const response = await app.inject({
      method: 'GET',
      url: '/metrics',
    });

    expect(response.statusCode).toBe(200);
    expect(response.headers['content-type']).toBe('text/plain; version=0.0.4; charset=utf-8');
    expect(response.payload).toContain('# HELP');
    expect(response.payload).toContain('# TYPE');
  });

  test('workflow metrics are recorded correctly', async () => {
    const metrics = new PrometheusMetricsCollector();
    const workflowMetrics = new WorkflowMetrics(metrics);

    const context: WorkflowContext = {
      workflowId: 'test-workflow',
      workflowType: 'autonomous-development',
      provider: 'anthropic-claude',
      platform: 'github',
    };

    workflowMetrics.recordWorkflowStart(context);

    const output = await metrics.getMetrics();
    expect(output).toContain(
      'workflows_started_total{workflow_type="autonomous-development",provider="anthropic-claude",platform="github"} 1'
    );
    expect(output).toContain('active_autonomous_loops{workflow_type="autonomous-development"} 1');
  });
});
```

## Success Metrics

### Metrics Coverage

- [ ] All critical business processes instrumented
- [ ] System health metrics comprehensive
- [ ] Performance metrics captured
- [ ] Error metrics detailed and actionable
- [ ] Custom metrics for business KPIs

### Performance Impact

- [ ] <1% overhead on application performance
- [ ] Metrics collection <10ms per operation
- [ ] Memory usage <100MB for metrics storage
- [ ] Network bandwidth <1MB/minute for metrics export

### Operational Excellence

- [ ] Prometheus scraping working correctly
- [ ] Alerting rules configured and tested
- [ ] Metrics retention policies implemented
- [ ] Dashboard visualizations available
- [ ] Automated monitoring of metrics system

## Dependencies

### Core Dependencies

```json
{
  "prom-client": "^15.0.0",
  "express": "^4.18.2"
}
```

### Development Dependencies

```json
{
  "@types/prom-client": "^13.0.0",
  "supertest": "^6.3.3"
}
```

## Risks and Mitigations

### Performance Impact

- **Risk**: Metrics collection affects application performance
- **Mitigation**: Async collection, sampling, performance monitoring

### High Cardinality

- **Risk**: Too many label combinations overwhelm storage
- **Mitigation**: Label limits, cardinality monitoring, aggregation

### Data Loss

- **Risk**: Metrics lost during system restarts
- **Mitigation**: Persistent storage, backup mechanisms

### Alert Fatigue

- **Risk**: Too many alerts reduce effectiveness
- **Mitigation**: Alert thresholds, grouping, escalation policies

## Rollout Plan

### Phase 1: Core Metrics (Week 1)

1. Prometheus client integration
2. Basic metrics registration
3. HTTP endpoint implementation

### Phase 2: Business Metrics (Week 2)

1. Workflow metrics integration
2. Provider and platform metrics
3. Error and performance metrics

### Phase 3: Advanced Features (Week 3)

1. Metrics aggregation
2. Custom dashboards
3. Alerting rules

### Phase 4: Production Deployment (Week 4)

1. Prometheus configuration
2. Monitoring setup
3. Performance optimization
4. Documentation and training

## Completion Criteria

- [ ] Prometheus metrics collector implemented
- [ ] All business metrics defined and collected
- [ ] HTTP metrics endpoint functional
- [ ] Performance overhead under 1%
- [ ] Prometheus scraping configured
- [ ] Alerting rules implemented
- [ ] Dashboards created and operational
- [ ] Comprehensive test coverage
- [ ] Documentation completed
- [ ] Production deployment successful

---

**Story Context**: This story implements comprehensive metrics collection infrastructure using Prometheus, providing detailed insights into system performance, business operations, and error patterns to enable effective monitoring and alerting for the Tamma platform.
