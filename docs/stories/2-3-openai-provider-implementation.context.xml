<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>2</epicId>
    <storyId>3</storyId>
    <title>OpenAI Provider Implementation</title>
    <status>ready-for-dev</status>
    <generatedAt>2025-11-08</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>/home/meywd/tamma/test-platform/docs/stories/2-3-openai-provider-implementation.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>benchmark runner</asA>
    <iWant>execute tasks using OpenAI models</iWant>
    <soThat>I can include GPT models in our benchmark evaluations</soThat>
    <tasks>Task 1: OpenAI SDK Integration (AC: 1)
- Subtask 1.1: Install and configure OpenAI SDK
- Subtask 1.2: Implement authentication with API key
- Subtask 1.3: Add organization and base URL support
Task 2: Model Support Implementation (AC: 2)
- Subtask 2.1: Implement GPT-4 model support
- Subtask 2.2: Implement GPT-4 Turbo model support
- Subtask 2.3: Implement GPT-3.5 Turbo model support
Task 3: Streaming Response Handling (AC: 3)
- Subtask 3.1: Implement streaming chat completion
- Subtask 3.2: Add chunked processing for real-time updates
- Subtask 3.3: Handle streaming errors and reconnection
Task 4: Token and Cost Management (AC: 4)
- Subtask 4.1: Implement token counting for requests/responses
- Subtask 4.2: Add cost calculation based on OpenAI pricing
- Subtask 4.3: Track usage metrics for billing
Task 5: Rate Limiting and Retry Logic (AC: 5)
- Subtask 5.1: Implement rate limit detection
- Subtask 5.2: Add exponential backoff retry mechanism
- Subtask 5.3: Handle quota exceeded scenarios
Task 6: Function Calling Support (AC: 6)
- Subtask 6.1: Implement function calling interface
- Subtask 6.2: Add function definition validation
- Subtask 6.3: Handle function call responses
Task 7: Configuration Management (AC: 7)
- Subtask 7.1: Add model parameter configuration
- Subtask 7.2: Implement system prompt handling
- Subtask 7.3: Add temperature and other parameter controls
Task 8: Error Handling (AC: 8)
- Subtask 8.1: Implement API error classification
- Subtask 8.2: Add timeout and connection error handling
- Subtask 8.3: Handle rate limit and quota errors gracefully</tasks>
  </story>

  <acceptanceCriteria>1. OpenAI SDK integration with proper authentication
2. Support for GPT-4, GPT-4 Turbo, and GPT-3.5 Turbo models
3. Streaming response handling with chunked processing
4. Token counting and cost calculation
5. Rate limiting and retry logic with exponential backoff
6. Support for function calling if needed for code tasks
7. Configuration for model parameters and system prompts
8. Error handling for API limits and failures</acceptanceCriteria>

  <artifacts>
    <docs>
      <doc path="docs/tech-spec-epic-1.md" title="Epic Technical Specification: Foundation & Core Infrastructure" section="Additional AI Provider Implementations" snippet="OpenAI Provider: OpenAIProvider implements IAIProvider - supports GPT-4, GPT-3.5-turbo, o1 models via openai@^4.67.0 SDK" />
      <doc path="docs/tech-spec-epic-1.md" title="Epic Technical Specification: Foundation & Core Infrastructure" section="AI Provider SDKs" snippet="openai@^4.67.0: OpenAI GPT-4, GPT-3.5-turbo, o1 models" />
      <doc path="docs/epics.md" title="Epic Breakdown with 58 Stories" section="Story 2-3: OpenAI Provider Implementation" snippet="1. OpenAI provider implements IAIProvider interface with support for GPT-4, GPT-3.5-turbo, and o1 models" />
      <doc path="docs/stories/1-10-additional-ai-provider-implementations.md" title="Additional AI Provider Implementations" section="Task 1: OpenAI Provider Implementation" snippet="Create OpenAIProvider class implementing IAIProvider" />
      <doc path=".dev/spikes/providers/openai-provider.ts" title="OpenAI Provider Spike Implementation" snippet="Reference implementation with GPT-4o, GPT-4o-mini, GPT-4-turbo support" />
      <doc path=".dev/spikes/providers/base-provider.ts" title="Base Provider Interface" snippet="BaseProvider abstract class and TestResult interface" />
    </docs>
    <code>
      <file path="packages/providers/src/implementations/openai-provider.ts" type="implementation" description="OpenAI provider implementation with IAIProvider interface" />
      <file path="packages/providers/src/configs/openai-config.schema.ts" type="configuration" description="OpenAI-specific configuration schema" />
      <file path="packages/providers/src/types/openai-types.ts" type="types" description="OpenAI-specific type definitions" />
      <file path="packages/providers/src/utils/token-counter.ts" type="utility" description="Token counting utilities for OpenAI models" />
      <file path="packages/providers/src/utils/cost-calculator.ts" type="utility" description="Cost calculation based on OpenAI pricing" />
    </code>
    <dependencies>
      <dependency ecosystem="Node.js" packages="openai@^4.77.3" reason="OpenAI SDK for GPT-4, GPT-3.5-turbo, and o1 models" />
      <dependency ecosystem="Node.js" packages="@tamma/shared@workspace:*" reason="Shared types and utilities" />
      <dependency ecosystem="Node.js" packages="@tamma/observability@workspace:*" reason="Logging and monitoring" />
      <dependency ecosystem="Node.js" packages="tiktoken" reason="Accurate token counting for OpenAI models" />
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="technical" description="Must implement IAIProvider interface from packages/providers/src/types.ts" />
    <constraint type="performance" description="Streaming responses must support real-time chunked processing" />
    <constraint type="security" description="API keys must be handled securely with environment variable support" />
    <constraint type="reliability" description="Must implement exponential backoff retry with jitter" />
    <constraint type="compatibility" description="Must support OpenAI API v1 with proper error handling" />
    <constraint type="cost" description="Must track token usage and calculate costs accurately" />
    <constraint type="rate-limiting" description="Must respect OpenAI rate limits and handle 429 responses" />
  </constraints>
  <interfaces>
    <interface name="IAIProvider" location="packages/providers/src/types.ts" description="Main provider interface with initialize, sendMessage, sendMessageSync, getCapabilities, getModels, dispose methods" />
    <interface name="ProviderConfig" location="packages/providers/src/types.ts" description="Configuration interface with apiKey, baseUrl, timeout, maxRetries, model fields" />
    <interface name="MessageRequest" location="packages/providers/src/types.ts" description="Request interface with messages, model, temperature, tools, stream options" />
    <interface name="MessageResponse" location="packages/providers/src/types.ts" description="Response interface with content, usage, finishReason, tool_calls" />
    <interface name="MessageChunk" location="packages/providers/src/types.ts" description="Streaming chunk interface with delta, usage, finishReason" />
    <interface name="ProviderCapabilities" location="packages/providers/src/types.ts" description="Capabilities interface with streaming, images, tools, maxTokens" />
  </interfaces>
  <tests>
    <standards>
      <standard name="Unit Test Coverage" target="80% line, 75% branch, 85% function" description="Comprehensive test coverage for all provider methods" />
      <standard name="Integration Testing" requirement="Real API calls with test credentials" description="Test with OPENAI_API_KEY_TEST environment variable" />
      <standard name="Error Handling" requirement="100% coverage for error scenarios" description="Test rate limits, timeouts, invalid API keys, network errors" />
      <standard name="Performance Testing" requirement="p95 &lt; 500ms for API calls" description="Measure response times for different models" />
      <standard name="Streaming Tests" requirement="Real-time chunk processing" description="Test streaming with various content types and lengths" />
    </standards>
    <locations>
      <location path="packages/providers/src/implementations/__tests__/openai-provider.test.ts" type="unit" description="Unit tests for OpenAI provider implementation" />
      <location path="packages/providers/src/implementations/__tests__/openai-provider.integration.test.ts" type="integration" description="Integration tests with real OpenAI API" />
      <location path="packages/providers/src/utils/__tests__/token-counter.test.ts" type="unit" description="Tests for token counting utilities" />
      <location path="packages/providers/src/utils/__tests__/cost-calculator.test.ts" type="unit" description="Tests for cost calculation utilities" />
    </locations>
    <ideas>
      <test idea="Mock OpenAI API responses using MSW for consistent unit testing" />
      <test idea="Test rate limiting with simulated 429 responses and retry logic" />
      <test idea="Validate streaming with large responses and connection interruptions" />
      <test idea="Test function calling with various tool definitions and responses" />
      <test idea="Performance tests comparing sync vs streaming response times" />
      <test idea="Cost calculation accuracy tests with known token counts" />
      <test idea="Error classification tests for different OpenAI error codes" />
      <test idea="Configuration validation tests with invalid parameters" />
    </ideas>
  </tests>
</story-context>