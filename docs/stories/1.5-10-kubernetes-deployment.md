# Story 1.5-10: Kubernetes Deployment (Optional)

## Context

For enterprise deployments and large-scale operations, Tamma needs to support Kubernetes deployment. This provides scalability, high availability, and integration with existing Kubernetes infrastructure. This is an optional story for organizations requiring container orchestration.

## Technical Specification

### 1. Kubernetes Architecture

#### 1.1 Deployment Architecture

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: tamma
  labels:
    name: tamma
    app.kubernetes.io/name: tamma
    app.kubernetes.io/version: '1.0.0'

---

## ‚ö†Ô∏è MANDATORY: Before You Code

**ALL contributors MUST read and follow the comprehensive development process:**

üìñ **[BEFORE_YOU_CODE.md](../../BEFORE_YOU_CODE.md)**

This mandatory guide includes:
- 7-Phase Development Workflow (Read ‚Üí Research ‚Üí Break Down ‚Üí TDD ‚Üí Quality Gates ‚Üí Failure Handling)
- Knowledge Base Usage (.dev/ directory: spikes, bugs, findings, decisions)
- TRACE/DEBUG Logging Requirements for all functions
- Test-Driven Development (TDD) mandatory workflow
- 100% Test Coverage requirement
- Build Success enforcement
- Automatic retry and developer alert procedures

**Failure to follow this process will result in rework.**

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: tamma-config
  namespace: tamma
data:
  config.yaml: |
    # Tamma Configuration for Kubernetes
    server:
      mode: "service"
      host: "0.0.0.0"
      port: 3000
      
    database:
      host: "tamma-postgres"
      port: 5432
      database: "tamma"
      ssl: false
      
    redis:
      host: "tamma-redis"
      port: 6379
      
    providers:
      anthropic:
        apiKey: "${ANTHROPIC_API_KEY}"
        model: "claude-3-sonnet-20240229"
        
    platforms:
      github:
        token: "${GITHUB_TOKEN}"
        
    logging:
      level: "info"
      format: "json"
      
    observability:
      metrics:
        enabled: true
        port: 9090
      tracing:
        enabled: true
        endpoint: "http://jaeger:14268/api/traces"

---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: tamma-secrets
  namespace: tamma
type: Opaque
data:
  # Base64 encoded values
  database-password: <base64-encoded-password>
  anthropic-api-key: <base64-encoded-api-key>
  github-token: <base64-encoded-token>
  jwt-secret: <base64-encoded-jwt-secret>
```

#### 1.2 PostgreSQL Database

```yaml
# k8s/postgres.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: tamma-postgres
  namespace: tamma
spec:
  serviceName: tamma-postgres
  replicas: 1
  selector:
    matchLabels:
      app: tamma-postgres
  template:
    metadata:
      labels:
        app: tamma-postgres
    spec:
      containers:
        - name: postgres
          image: postgres:17-alpine
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_DB
              value: 'tamma'
            - name: POSTGRES_USER
              value: 'tamma'
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: tamma-secrets
                  key: database-password
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: '256Mi'
              cpu: '250m'
            limits:
              memory: '1Gi'
              cpu: '500m'
          livenessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - tamma
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            exec:
              command:
                - pg_isready
                - -U
                - tamma
            initialDelaySeconds: 5
            periodSeconds: 5
  volumeClaimTemplates:
    - metadata:
        name: postgres-storage
      spec:
        accessModes: ['ReadWriteOnce']
        resources:
          requests:
            storage: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: tamma-postgres
  namespace: tamma
spec:
  selector:
    app: tamma-postgres
  ports:
    - port: 5432
      targetPort: 5432
  type: ClusterIP
```

#### 1.3 Redis Cache

```yaml
# k8s/redis.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tamma-redis
  namespace: tamma
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tamma-redis
  template:
    metadata:
      labels:
        app: tamma-redis
    spec:
      containers:
        - name: redis
          image: redis:7-alpine
          ports:
            - containerPort: 6379
          resources:
            requests:
              memory: '128Mi'
              cpu: '100m'
            limits:
              memory: '512Mi'
              cpu: '200m'
          livenessProbe:
            exec:
              command:
                - redis-cli
                - ping
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            exec:
              command:
                - redis-cli
                - ping
            initialDelaySeconds: 5
            periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: tamma-redis
  namespace: tamma
spec:
  selector:
    app: tamma-redis
  ports:
    - port: 6379
      targetPort: 6379
  type: ClusterIP
```

### 2. Tamma Application Deployment

#### 2.1 Orchestrator Service

```yaml
# k8s/orchestrator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tamma-orchestrator
  namespace: tamma
  labels:
    app: tamma-orchestrator
    app.kubernetes.io/name: tamma
    app.kubernetes.io/component: orchestrator
spec:
  replicas: 2
  selector:
    matchLabels:
      app: tamma-orchestrator
  template:
    metadata:
      labels:
        app: tamma-orchestrator
        app.kubernetes.io/name: tamma
        app.kubernetes.io/component: orchestrator
    spec:
      containers:
        - name: orchestrator
          image: tamma/orchestrator:1.0.0
          ports:
            - containerPort: 3000
            - containerPort: 9090 # metrics
          env:
            - name: NODE_ENV
              value: 'production'
            - name: DATABASE_URL
              value: 'postgresql://tamma:$(DATABASE_PASSWORD)@tamma-postgres:5432/tamma'
            - name: REDIS_URL
              value: 'redis://tamma-redis:6379'
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: tamma-secrets
                  key: database-password
            - name: ANTHROPIC_API_KEY
              valueFrom:
                secretKeyRef:
                  name: tamma-secrets
                  key: anthropic-api-key
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: tamma-secrets
                  key: github-token
            - name: JWT_SECRET
              valueFrom:
                secretKeyRef:
                  name: tamma-secrets
                  key: jwt-secret
          volumeMounts:
            - name: config
              mountPath: /app/config
          resources:
            requests:
              memory: '512Mi'
              cpu: '500m'
            limits:
              memory: '2Gi'
              cpu: '1000m'
          livenessProbe:
            httpGet:
              path: /health
              port: 3000
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 3000
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
      volumes:
        - name: config
          configMap:
            name: tamma-config

---
apiVersion: v1
kind: Service
metadata:
  name: tamma-orchestrator
  namespace: tamma
  labels:
    app: tamma-orchestrator
spec:
  selector:
    app: tamma-orchestrator
  ports:
    - name: http
      port: 3000
      targetPort: 3000
    - name: metrics
      port: 9090
      targetPort: 9090
  type: ClusterIP
```

#### 2.2 Worker Deployment

```yaml
# k8s/worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tamma-worker
  namespace: tamma
  labels:
    app: tamma-worker
    app.kubernetes.io/name: tamma
    app.kubernetes.io/component: worker
spec:
  replicas: 3
  selector:
    matchLabels:
      app: tamma-worker
  template:
    metadata:
      labels:
        app: tamma-worker
        app.kubernetes.io/name: tamma
        app.kubernetes.io/component: worker
    spec:
      containers:
        - name: worker
          image: tamma/workers:1.0.0
          env:
            - name: NODE_ENV
              value: 'production'
            - name: DATABASE_URL
              value: 'postgresql://tamma:$(DATABASE_PASSWORD)@tamma-postgres:5432/tamma'
            - name: REDIS_URL
              value: 'redis://tamma-redis:6379'
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: tamma-secrets
                  key: database-password
            - name: ANTHROPIC_API_KEY
              valueFrom:
                secretKeyRef:
                  name: tamma-secrets
                  key: anthropic-api-key
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: tamma-secrets
                  key: github-token
          volumeMounts:
            - name: config
              mountPath: /app/config
          resources:
            requests:
              memory: '256Mi'
              cpu: '250m'
            limits:
              memory: '1Gi'
              cpu: '500m'
          livenessProbe:
            exec:
              command:
                - node
                - -e
                - "require('@tamma/healthcheck').check()"
            initialDelaySeconds: 60
            periodSeconds: 30
          readinessProbe:
            exec:
              command:
                - node
                - -e
                - "require('@tamma/healthcheck').check()"
            initialDelaySeconds: 30
            periodSeconds: 10
      volumes:
        - name: config
          configMap:
            name: tamma-config
```

#### 2.3 API Gateway

```yaml
# k8s/api.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tamma-api
  namespace: tamma
  labels:
    app: tamma-api
    app.kubernetes.io/name: tamma
    app.kubernetes.io/component: api
spec:
  replicas: 2
  selector:
    matchLabels:
      app: tamma-api
  template:
    metadata:
      labels:
        app: tamma-api
        app.kubernetes.io/name: tamma
        app.kubernetes.io/component: api
    spec:
      containers:
        - name: api
          image: tamma/api:1.0.0
          ports:
            - containerPort: 8080
          env:
            - name: NODE_ENV
              value: 'production'
            - name: DATABASE_URL
              value: 'postgresql://tamma:$(DATABASE_PASSWORD)@tamma-postgres:5432/tamma'
            - name: REDIS_URL
              value: 'redis://tamma-redis:6379'
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: tamma-secrets
                  key: database-password
            - name: JWT_SECRET
              valueFrom:
                secretKeyRef:
                  name: tamma-secrets
                  key: jwt-secret
          volumeMounts:
            - name: config
              mountPath: /app/config
          resources:
            requests:
              memory: '256Mi'
              cpu: '250m'
            limits:
              memory: '1Gi'
              cpu: '500m'
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
      volumes:
        - name: config
          configMap:
            name: tamma-config

---
apiVersion: v1
kind: Service
metadata:
  name: tamma-api
  namespace: tamma
  labels:
    app: tamma-api
spec:
  selector:
    app: tamma-api
  ports:
    - port: 8080
      targetPort: 8080
  type: ClusterIP
```

### 3. Ingress and Load Balancing

#### 3.1 Ingress Configuration

```yaml
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tamma-ingress
  namespace: tamma
  annotations:
    kubernetes.io/ingress.class: 'nginx'
    nginx.ingress.kubernetes.io/ssl-redirect: 'true'
    nginx.ingress.kubernetes.io/use-regex: 'true'
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    cert-manager.io/cluster-issuer: 'letsencrypt-prod'
    nginx.ingress.kubernetes.io/rate-limit: '100'
    nginx.ingress.kubernetes.io/rate-limit-window: '1m'
spec:
  tls:
    - hosts:
        - tamma.example.com
      secretName: tamma-tls
  rules:
    - host: tamma.example.com
      http:
        paths:
          - path: /api(/|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: tamma-api
                port:
                  number: 8080
          - path: /webhooks(/|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: tamma-api
                port:
                  number: 8080
          - path: /
            pathType: Prefix
            backend:
              service:
                name: tamma-orchestrator
                port:
                  number: 3000

---
apiVersion: v1
kind: Service
metadata:
  name: tamma-orchestrator-public
  namespace: tamma
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: 'nlb'
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: 'true'
spec:
  selector:
    app: tamma-orchestrator
  ports:
    - port: 3000
      targetPort: 3000
  type: LoadBalancer
```

### 4. Monitoring and Observability

#### 4.1 Prometheus Monitoring

```yaml
# k8s/monitoring.yaml
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: tamma-metrics
  namespace: tamma
  labels:
    app: tamma
spec:
  selector:
    matchLabels:
      app: tamma-orchestrator
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics

---
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: tamma-api-metrics
  namespace: tamma
  labels:
    app: tamma
spec:
  selector:
    matchLabels:
      app: tamma-api
  endpoints:
    - port: http
      interval: 30s
      path: /metrics

---
apiVersion: v1
kind: PodMonitor
metadata:
  name: tamma-worker-metrics
  namespace: tamma
  labels:
    app: tamma
spec:
  selector:
    matchLabels:
      app: tamma-worker
  endpoints:
    - port: metrics
      interval: 30s
```

#### 4.2 Logging Configuration

```yaml
# k8s/logging.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: tamma
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/*tamma*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>

    <match kubernetes.**>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      index_name tamma-logs
      type_name _doc
    </match>

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: tamma
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      containers:
        - name: fluentd
          image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
          env:
            - name: FLUENT_ELASTICSEARCH_HOST
              value: 'elasticsearch.logging.svc.cluster.local'
            - name: FLUENT_ELASTICSEARCH_PORT
              value: '9200'
          volumeMounts:
            - name: varlog
              mountPath: /var/log
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
            - name: fluentd-config
              mountPath: /fluentd/etc
      volumes:
        - name: varlog
          hostPath:
            path: /var/log
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: fluentd-config
          configMap:
            name: fluentd-config
```

### 5. Autoscaling and Resource Management

#### 5.1 Horizontal Pod Autoscaler

```yaml
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: tamma-orchestrator-hpa
  namespace: tamma
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tamma-orchestrator
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: tamma-worker-hpa
  namespace: tamma
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tamma-worker
  minReplicas: 3
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 85
```

#### 5.2 Resource Quotas

```yaml
# k8s/resource-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tamma-quota
  namespace: tamma
spec:
  hard:
    requests.cpu: '10'
    requests.memory: 20Gi
    limits.cpu: '20'
    limits.memory: 40Gi
    persistentvolumeclaims: '10'
    pods: '50'
    services: '20'
    secrets: '20'
    configmaps: '20'

---
apiVersion: v1
kind: LimitRange
metadata:
  name: tamma-limits
  namespace: tamma
spec:
  limits:
    - default:
        cpu: '500m'
        memory: '1Gi'
      defaultRequest:
        cpu: '100m'
        memory: '128Mi'
      type: Container
    - max:
        cpu: '2'
        memory: '4Gi'
      min:
        cpu: '50m'
        memory: '64Mi'
      type: Container
```

### 6. Security and Network Policies

#### 6.1 Network Policies

```yaml
# k8s/network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tamma-network-policy
  namespace: tamma
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
        - podSelector:
            matchLabels:
              app: tamma-api
        - podSelector:
            matchLabels:
              app: tamma-orchestrator
        - podSelector:
            matchLabels:
              app: tamma-worker
      ports:
        - protocol: TCP
          port: 3000
        - protocol: TCP
          port: 8080
  egress:
    - to:
        - podSelector:
            matchLabels:
              app: tamma-postgres
      ports:
        - protocol: TCP
          port: 5432
    - to:
        - podSelector:
            matchLabels:
              app: tamma-redis
      ports:
        - protocol: TCP
          port: 6379
    - to: []
      ports:
        - protocol: TCP
          port: 443 # HTTPS for external APIs
        - protocol: TCP
          port: 53 # DNS
        - protocol: UDP
          port: 53 # DNS
```

#### 6.2 Pod Security Policy

```yaml
# k8s/pod-security-policy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: tamma-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: true
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
```

### 7. Deployment Automation

#### 7.1 Helm Chart

```yaml
# helm/tamma/Chart.yaml
apiVersion: v2
name: tamma
description: Tamma - Autonomous development orchestration platform
type: application
version: 1.0.0
appVersion: '1.0.0'
keywords:
  - ai
  - development
  - automation
  - orchestration
home: https://github.com/tamma/tamma
sources:
  - https://github.com/tamma/tamma
maintainers:
  - name: Tamma Team
    email: team@tamma.dev

# helm/tamma/values.yaml
global:
  imageRegistry: ''
  imagePullSecrets: []
  storageClass: ''

image:
  registry: docker.io
  repository: tamma
  tag: '1.0.0'
  pullPolicy: IfNotPresent

replicaCount:
  orchestrator: 2
  worker: 3
  api: 2

resources:
  orchestrator:
    requests:
      memory: '512Mi'
      cpu: '500m'
    limits:
      memory: '2Gi'
      cpu: '1000m'
  worker:
    requests:
      memory: '256Mi'
      cpu: '250m'
    limits:
      memory: '1Gi'
      cpu: '500m'
  api:
    requests:
      memory: '256Mi'
      cpu: '250m'
    limits:
      memory: '1Gi'
      cpu: '500m'

autoscaling:
  enabled: true
  orchestrator:
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
  worker:
    minReplicas: 3
    maxReplicas: 20
    targetCPUUtilizationPercentage: 80

ingress:
  enabled: true
  className: 'nginx'
  annotations:
    cert-manager.io/cluster-issuer: 'letsencrypt-prod'
  host: tamma.example.com
  tls:
    enabled: true
    secretName: tamma-tls

database:
  postgres:
    enabled: true
    image: postgres:17-alpine
    storage: 10Gi
  redis:
    enabled: true
    image: redis:7-alpine

monitoring:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 30s

security:
  networkPolicy:
    enabled: true
  podSecurityPolicy:
    enabled: true
```

#### 7.2 Deployment Scripts

```bash
#!/bin/bash
# scripts/deploy-k8s.sh

set -e

# Configuration
NAMESPACE=${NAMESPACE:-"tamma"}
ENVIRONMENT=${ENVIRONMENT:-"production"}
HELM_CHART_PATH="helm/tamma"
VALUES_FILE="helm/tamma/values-${ENVIRONMENT}.yaml"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check prerequisites
check_prerequisites() {
    log_info "Checking prerequisites..."

    if ! command -v kubectl >/dev/null 2>&1; then
        log_error "kubectl is required but not installed"
        exit 1
    fi

    if ! command -v helm >/dev/null 2>&1; then
        log_error "helm is required but not installed"
        exit 1
    fi

    # Check cluster connection
    if ! kubectl cluster-info >/dev/null 2>&1; then
        log_error "Cannot connect to Kubernetes cluster"
        exit 1
    fi

    log_info "Prerequisites check passed"
}

# Create namespace
create_namespace() {
    log_info "Creating namespace: $NAMESPACE"

    kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -

    log_info "Namespace created/verified"
}

# Deploy secrets
deploy_secrets() {
    log_info "Deploying secrets..."

    # Create secrets from environment variables
    kubectl create secret generic tamma-secrets \
        --from-literal=database-password="$DATABASE_PASSWORD" \
        --from-literal=anthropic-api-key="$ANTHROPIC_API_KEY" \
        --from-literal=github-token="$GITHUB_TOKEN" \
        --from-literal=jwt-secret="$JWT_SECRET" \
        --namespace="$NAMESPACE" \
        --dry-run=client -o yaml | kubectl apply -f -

    log_info "Secrets deployed"
}

# Deploy with Helm
deploy_helm() {
    log_info "Deploying Tamma with Helm..."

    # Add Helm repository if needed
    helm repo add tamma https://charts.tamma.dev || true
    helm repo update

    # Deploy or upgrade
    if helm release list --namespace "$NAMESPACE" | grep -q "tamma"; then
        log_info "Upgrading existing deployment"
        helm upgrade tamma "$HELM_CHART_PATH" \
            --namespace "$NAMESPACE" \
            --values "$VALUES_FILE" \
            --wait \
            --timeout 10m
    else
        log_info "Installing new deployment"
        helm install tamma "$HELM_CHART_PATH" \
            --namespace "$NAMESPACE" \
            --values "$VALUES_FILE" \
            --wait \
            --timeout 10m
    fi

    log_info "Helm deployment completed"
}

# Verify deployment
verify_deployment() {
    log_info "Verifying deployment..."

    # Wait for pods to be ready
    kubectl wait --for=condition=ready pod \
        --namespace "$NAMESPACE" \
        --all \
        --timeout=300s

    # Check pod status
    kubectl get pods --namespace "$NAMESPACE"

    # Check services
    kubectl get services --namespace "$NAMESPACE"

    # Check ingress if enabled
    if kubectl get ingress --namespace "$NAMESPACE" >/dev/null 2>&1; then
        kubectl get ingress --namespace "$NAMESPACE"
    fi

    log_info "Deployment verification completed"
}

# Show access information
show_access_info() {
    log_info "Access Information:"

    # Get ingress URL
    if kubectl get ingress tamma-ingress --namespace "$NAMESPACE" >/dev/null 2>&1; then
        ingress_host=$(kubectl get ingress tamma-ingress --namespace "$NAMESPACE" -o jsonpath='{.spec.rules[0].host}')
        log_info "Web UI: https://$ingress_host"
        log_info "API: https://$ingress_host/api"
    fi

    # Get load balancer URL
    if kubectl get service tamma-orchestrator-public --namespace "$NAMESPACE" >/dev/null 2>&1; then
        lb_ip=$(kubectl get service tamma-orchestrator-public --namespace "$NAMESPACE" -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
        if [ -n "$lb_ip" ]; then
            log_info "Load Balancer IP: $lb_ip:3000"
        fi
    fi

    # Port forwarding info
    log_info "For local access, use port forwarding:"
    log_info "kubectl port-forward service/tamma-orchestrator 3000:3000 -n $NAMESPACE"
    log_info "kubectl port-forward service/tamma-api 8080:8080 -n $NAMESPACE"
}

# Main deployment
main() {
    log_info "Starting Tamma Kubernetes deployment..."
    log_info "Namespace: $NAMESPACE"
    log_info "Environment: $ENVIRONMENT"

    check_prerequisites
    create_namespace
    deploy_secrets
    deploy_helm
    verify_deployment
    show_access_info

    log_info "üéâ Tamma deployment completed successfully!"
}

# Handle cleanup on exit
cleanup() {
    log_info "Cleaning up..."
}

trap cleanup EXIT

# Run deployment
main "$@"
```

## Implementation Tasks

### Phase 1: Core Kubernetes Setup (Week 1)

1. **Basic Deployment**
   - Create namespace and basic configurations
   - Deploy PostgreSQL and Redis
   - Set up basic Tamma services

2. **Networking**
   - Configure services and ingress
   - Set up load balancing
   - Implement network policies

### Phase 2: Monitoring and Scaling (Week 2)

1. **Observability**
   - Deploy Prometheus monitoring
   - Set up logging with Fluentd
   - Configure health checks

2. **Autoscaling**
   - Implement HPA for all services
   - Configure resource limits
   - Set up cluster autoscaling

### Phase 3: Security and Automation (Week 3)

1. **Security**
   - Implement network policies
   - Set up RBAC
   - Configure pod security policies

2. **Automation**
   - Create Helm chart
   - Set up deployment scripts
   - Configure CI/CD integration

## Acceptance Criteria

### Functional Requirements

- [ ] Tamma deploys successfully on Kubernetes
- [ ] All components (orchestrator, workers, API) running
- [ ] Database and Redis properly configured
- [ ] Load balancing and ingress working
- [ ] Autoscaling functional

### Non-Functional Requirements

- [ ] High availability with multiple replicas
- [ ] Resource limits and quotas enforced
- [ ] Security policies implemented
- [ ] Monitoring and logging configured
- [ ] Automated deployment via Helm

### Integration Requirements

- [ ] External API access (GitHub, AI providers)
- [ ] Webhook ingress configuration
- [ ] Persistent storage for database
- [ ] Backup and recovery procedures
- [ ] Multi-environment support

## Testing Strategy

### Unit Tests

- Kubernetes manifest validation
- Helm chart template testing
- Configuration validation

### Integration Tests

- End-to-end deployment testing
- Service connectivity testing
- Autoscaling behavior testing

### Performance Tests

- Load testing with multiple replicas
- Resource utilization testing
- Failover and recovery testing

## Dependencies

### Internal Dependencies

- All Epic 1.5 stories (1.5-1 through 1.5-9)
- Docker images for all services
- Configuration management

### External Dependencies

- Kubernetes cluster (v1.25+)
- Helm package manager
- Ingress controller
- Monitoring stack (Prometheus, Grafana)

## Risks and Mitigations

### Technical Risks

- **Cluster resource constraints**: Implement resource quotas and monitoring
- **Network connectivity issues**: Comprehensive network policies and testing
- **Storage failures**: Implement backup and recovery procedures

### Operational Risks

- **Deployment complexity**: Use Helm charts and automation
- **Configuration drift**: GitOps and configuration management
- **Security vulnerabilities**: Regular security scans and updates

## Success Metrics

### Deployment Metrics

- Deployment success rate: > 99%
- Deployment time: < 10 minutes
- Service availability: > 99.9%

### Performance Metrics

- Pod startup time: < 60 seconds
- Autoscaling response time: < 2 minutes
- Resource utilization efficiency: > 80%

### Reliability Metrics

- Mean time to recovery (MTTR): < 5 minutes
- Service level agreement (SLA): 99.9% uptime
- Incident response time: < 15 minutes

## Rollout Plan

### Phase 1: Development Environment

- Deploy to development cluster
- Test all components and configurations
- Validate monitoring and alerting

### Phase 2: Staging Environment

- Deploy to staging cluster
- Performance and load testing
- Security validation

### Phase 3: Production Environment

- Deploy to production cluster
- Monitor performance and reliability
- Optimize configurations

## Monitoring and Alerting

### Application Metrics

- Request latency and error rates
- Task processing throughput
- Resource utilization

### Infrastructure Metrics

- Pod health and restarts
- Node resource utilization
- Network performance

### Business Metrics

- Autonomous task completion rate
- User satisfaction scores
- System reliability metrics

### References

- **üî¥ MANDATORY PROCESS:** [BEFORE_YOU_CODE.md](../../BEFORE_YOU_CODE.md)
- **Knowledge Base:** [.dev/README.md](../../.dev/README.md) - Search spikes, bugs, findings, decisions
