<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>2</epicId>
    <storyId>3</storyId>
    <title>OpenAI Provider Implementation</title>
    <status>drafted</status>
    <generatedAt>2025-11-06T12:00:00.000Z</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/2-3-openai-provider-implementation.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>As a benchmark runner,</asA>
    <iWant>I want to execute tasks using OpenAI models,</iWant>
    <soThat>so that I can include GPT models in our benchmark evaluations.</soThat>
    <tasks>- [ ] Task 1: OpenAI SDK Integration (AC: 1)
  - [ ] Subtask 1.1: Install and configure OpenAI SDK
  - [ ] Subtask 1.2: Implement authentication with API key
  - [ ] Subtask 1.3: Add organization and base URL support
- [ ] Task 2: Model Support Implementation (AC: 2)
  - [ ] Subtask 2.1: Implement GPT-4 model support
  - [ ] Subtask 2.2: Implement GPT-4 Turbo model support
  - [ ] Subtask 2.3: Implement GPT-3.5 Turbo model support
- [ ] Task 3: Streaming Response Handling (AC: 3)
  - [ ] Subtask 3.1: Implement streaming chat completion
  - [ ] Subtask 3.2: Add chunked processing for real-time updates
  - [ ] Subtask 3.3: Handle streaming errors and reconnection
- [ ] Task 4: Token and Cost Management (AC: 4)
  - [ ] Subtask 4.1: Implement token counting for requests/responses
  - [ ] Subtask 4.2: Add cost calculation based on OpenAI pricing
  - [ ] Subtask 4.3: Track usage metrics for billing
- [ ] Task 5: Rate Limiting and Retry Logic (AC: 5)
  - [ ] Subtask 5.1: Implement rate limit detection
  - [ ] Subtask 5.2: Add exponential backoff retry mechanism
  - [ ] Subtask 5.3: Handle quota exceeded scenarios
- [ ] Task 6: Function Calling Support (AC: 6)
  - [ ] Subtask 6.1: Implement function calling interface
  - [ ] Subtask 6.2: Add function definition validation
  - [ ] Subtask 6.3: Handle function call responses
- [ ] Task 7: Configuration Management (AC: 7)
  - [ ] Subtask 7.1: Add model parameter configuration
  - [ ] Subtask 7.2: Implement system prompt handling
  - [ ] Subtask 7.3: Add temperature and other parameter controls
- [ ] Task 8: Error Handling (AC: 8)
  - [ ] Subtask 8.1: Implement API error classification
  - [ ] Subtask 8.2: Add timeout and connection error handling
  - [ ] Subtask 8.3: Handle rate limit and quota errors gracefully</tasks>
  </story>

  <acceptanceCriteria>1. OpenAI SDK integration with proper authentication
2. Support for GPT-4, GPT-4 Turbo, and GPT-3.5 Turbo models
3. Streaming response handling with chunked processing
4. Token counting and cost calculation
5. Rate limiting and retry logic with exponential backoff
6. Support for function calling if needed for code tasks
7. Configuration for model parameters and system prompts
8. Error handling for API limits and failures</acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/tech-spec-epic-2.md</path>
        <title>Epic Technical Specification: AI Provider Integration</title>
        <section>OpenAI Provider Implementation</section>
        <snippet>OpenAI Provider Implementation with comprehensive interface definition, streaming support, and error handling patterns.</snippet>
      </doc>
      <doc>
        <path>docs/stories/2-1-ai-provider-abstraction-interface.md</path>
        <title>Story 2.1: AI Provider Abstraction Interface</title>
        <section>IAIProvider Interface Definition</section>
        <snippet>Core abstraction interface that OpenAI provider must implement with standardized methods for chat completion, streaming, and model management.</snippet>
      </doc>
      <doc>
        <path>docs/ARCHITECTURE.md</path>
        <title>AI Benchmarking as a Service (AIBaaS) - Architecture Document</title>
        <section>Provider Integration Layer</section>
        <snippet>System architecture showing provider abstraction layer between benchmark execution engine and external AI services.</snippet>
      </doc>
    </docs>
    <code>
      <!-- No existing code found - this is a new implementation -->
    </code>
    <dependencies>
      <ecosystem>Node.js/TypeScript</ecosystem>
      <packages>
        <package name="openai" version="latest" purpose="Official OpenAI SDK for API integration"/>
        <package name="typescript" version="5.7+" purpose="Type safety and interface definitions"/>
        <package name="@types/node" version="latest" purpose="Node.js type definitions"/>
      </packages>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>Must implement IAIProvider interface from Story 2.1</constraint>
    <constraint>Use TypeScript strict mode for type safety</constraint>
    <constraint>Follow async/await patterns for all API calls</constraint>
    <constraint>Implement proper error handling with custom provider errors</constraint>
    <constraint>Support streaming responses using AsyncIterable pattern</constraint>
    <constraint>Include circuit breaker pattern for API resilience</constraint>
    <constraint>Never log or expose API keys in error messages or logs</constraint>
    <constraint>Validate all configuration parameters on initialization</constraint>
  </constraints>

  <interfaces>
    <interface name="IAIProvider" kind="TypeScript Interface" signature="interface IAIProvider {
  readonly name: string;
  readonly version: string;
  readonly capabilities: ProviderCapabilities;
  initialize(config: ProviderConfig): Promise&lt;void&gt;;
  dispose(): Promise&lt;void&gt;;
  healthCheck(): Promise&lt;ProviderHealth&gt;;
  listModels(): Promise&lt;Model[]&gt;;
  getModel(modelId: string): Promise&lt;Model&gt;;
  createChatCompletion(request: ChatCompletionRequest): Promise&lt;ChatCompletionResponse&gt;;
  createChatCompletionStream(request: ChatCompletionRequest): AsyncIterable&lt;ChatCompletionChunk&gt;;
  createEmbedding(request: EmbeddingRequest): Promise&lt;EmbeddingResponse&gt;;
  countTokens(text: string, model?: string): Promise&lt;number&gt;;
  getRateLimitInfo(modelId?: string): Promise&lt;RateLimitInfo&gt;;
}" path="docs/stories/2-1-ai-provider-abstraction-interface.md"/>
    
    <interface name="OpenAIConfig" kind="TypeScript Interface" signature="interface OpenAIConfig extends ProviderConfig {
  type: 'openai';
  apiKey: string;
  organization?: string;
  baseURL?: string;
  timeout?: number;
  maxRetries?: number;
}" path="docs/tech-spec-epic-2.md"/>
    
    <interface name="ChatCompletionRequest" kind="TypeScript Interface" signature="interface ChatCompletionRequest {
  model: string;
  messages: ChatMessage[];
  temperature?: number;
  maxTokens?: number;
  topP?: number;
  frequencyPenalty?: number;
  presencePenalty?: number;
  stop?: string | string[];
  stream?: boolean;
  functions?: FunctionDefinition[];
  functionCall?: 'auto' | 'none' | { name: string };
  user?: string;
  metadata?: Record&lt;string, any&gt;;
}" path="docs/tech-spec-epic-2.md"/>
  </interfaces>

  <tests>
    <standards>Unit tests with Vitest framework, mocking external OpenAI API calls. Integration tests with real OpenAI test credentials. Performance tests for streaming and rate limiting. Error handling tests for all failure scenarios.</standards>
    <locations>tests/providers/openai-provider.test.ts for unit tests, tests/integration/openai-integration.test.ts for integration tests</locations>
    <ideas>
      <test idea="Test authentication with invalid API key should throw ProviderAuthenticationError" mapsTo="AC:1"/>
      <test idea="Test streaming chat completion returns AsyncIterable of chunks" mapsTo="AC:3"/>
      <test idea="Test token counting matches OpenAI's tiktoken calculation" mapsTo="AC:4"/>
      <test idea="Test rate limit detection triggers exponential backoff" mapsTo="AC:5"/>
      <test idea="Test function calling with valid function definitions" mapsTo="AC:6"/>
      <test idea="Test configuration validation rejects invalid parameters" mapsTo="AC:7"/>
      <test idea="Test error classification maps API errors to provider errors" mapsTo="AC:8"/>
    </ideas>
  </tests>
</story-context>